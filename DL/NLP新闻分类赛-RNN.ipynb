{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:18.467385Z",
     "start_time": "2025-01-23T13:58:14.663858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import logging\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')"
   ],
   "id": "c4302de61ce2000c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:18.869871Z",
     "start_time": "2025-01-23T13:58:18.473384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 预训练的词向量文件路径\n",
    "vec_path = \"word2vec.txt\"  # 替换为实际路径\n",
    "# 加载词向量文件\n",
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(vec_path, binary=False)\n",
    "\n",
    "# 获取原始词向量的维度\n",
    "embedding_dim = word_vectors.vector_size\n",
    "\n",
    "# 初始化词汇表，包含特殊标记\n",
    "vocab = {'<unk>': 0, '[PAD]': 1}  # <unk> 和 [PAD] 的索引分别为 0 和 1\n",
    "\n",
    "# 将原始词向量添加到词汇表中\n",
    "for idx, word in enumerate(word_vectors.index_to_key, start=len(vocab)):\n",
    "    vocab[word] = idx\n",
    "\n",
    "# 更新 word_vectors，确保包含特殊标记\n",
    "special_tokens = {'<unk>': np.random.uniform(-0.25, 0.25, embedding_dim),\n",
    "                  '[PAD]': np.zeros(embedding_dim)}\n",
    "\n",
    "for token, vector in special_tokens.items():\n",
    "    if token not in word_vectors:\n",
    "        word_vectors.add_vector(token, vector)\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))"
   ],
   "id": "e21608082ba31374",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 21:58:18,475 INFO: loading projection weights from word2vec.txt\n",
      "2025-01-23 21:58:18,863 INFO: KeyedVectors lifecycle event {'msg': 'loaded (5971, 200) matrix of type float32 from word2vec.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-01-23T21:58:18.863385', 'gensim': '4.3.3', 'python': '3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:48:34) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:19.026964Z",
     "start_time": "2025-01-23T13:58:19.023123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#数据集处理\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word_vectors, max_len, unk_token='<unk>'):\n",
    "        \"\"\"\n",
    "        文本数据集类\n",
    "        :param texts: 文本数据列表，每个元素是一个词索引列表。\n",
    "        :param labels: 标签列表，与 texts 中的文本一一对应。\n",
    "        :param word_vectors: 词向量对象，包含词汇表和对应的词向量。\n",
    "        :param max_len: 每个文本的最大长度，用于填充或截断文本。\n",
    "        :param unk_token: 未知词标记，默认为 '<unk>'.\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.word_vectors = word_vectors\n",
    "        self.unk_token = unk_token\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # 确保 <unk> 和 [PAD] 已经存在于 word_vectors 中\n",
    "        assert '<unk>' in self.word_vectors.key_to_index, \"Vocabulary does not contain '<unk>' token.\"\n",
    "        assert '[PAD]' in self.word_vectors.key_to_index, \"Vocabulary does not contain '[PAD]' token.\"\n",
    "\n",
    "        # 填充或截断文本\n",
    "        self.text = [self.pad_text(text, max_len) for text in texts]\n",
    "\n",
    "    def pad_text(self, text, max_len):\n",
    "        \"\"\"\n",
    "        填充或截断文本以匹配指定的最大长度\n",
    "        :param text: 文本数据列表，每个元素是一个词索引列表。\n",
    "        :param max_len: 每个文本的最大长度，用于填充或截断文本。\n",
    "        \"\"\"\n",
    "        if not isinstance(text, list):\n",
    "            raise ValueError(\"Expected a list of integers as input.\")\n",
    "\n",
    "        padded = text[:max_len]  # 截取前 max_len 个元素\n",
    "        padding_needed = max_len - len(padded)\n",
    "        if padding_needed > 0:\n",
    "            padded.extend([self.word_vectors.key_to_index.get('[PAD]', 0)] * padding_needed)  # 使用[PAD]索引填充\n",
    "        return padded\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集中样本的数量\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        获取指定索引的数据\n",
    "        :param index: 数据索引。\n",
    "        :return: 一个元组，包含文本数据和对应的标签。\n",
    "        \"\"\"\n",
    "        text_tensor = torch.tensor(self.text[index], dtype=torch.long)  # 只传递词索引\n",
    "        label_tensor = torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        return text_tensor, label_tensor"
   ],
   "id": "31a0557949a5765d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:22.214141Z",
     "start_time": "2025-01-23T13:58:22.210384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size,embedding_dim, hidden_dim, output_dim, num_layers, dropout):\n",
    "\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=True, batch_first=True,dropout=dropout if num_layers > 1 else 0)\n",
    "        # 双向的LSTM，所以hidden_dim*2\n",
    "        self.fc=nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        lstm_out, (hidden,cell) = self.lstm(embedded)\n",
    "        #合并双向LSTM的最后一个时间步的隐藏状态\n",
    "        # 如果num_layers > 1，则取最后一层的隐藏状态\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]),dim=1)\n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ],
   "id": "c9aed54a545cd0b0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:22.823804Z",
     "start_time": "2025-01-23T13:58:22.817798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model,dataloader,criterion,optimizer,num_epochs,scheduler=None):\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "    :param model: 模型。\n",
    "    :param dataloader: 数据加载器。\n",
    "    :param criterion: 损失函数。\n",
    "    :param optimizer: 优化器。\n",
    "    :param num_epochs: 训练轮数。\n",
    "    :param scheduler: 学习率调度器，默认为 None。\n",
    "    :return: 训练损失列表和准确率列表。\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logging.info('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        logging.info('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 训练模式\n",
    "            else:\n",
    "                model.eval()   # 评估模式\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad() #梯度清零\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'): #是否计算梯度\n",
    "                    outputs = model(inputs) #前向传播\n",
    "                    _, preds = torch.max(outputs, 1) #预测结果\n",
    "                    loss = criterion(outputs, labels) #损失函数\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        # 手动添加L2正则化损失\n",
    "                        # l2_loss = 0.0\n",
    "                        # for param in model.parameters():\n",
    "                        #     if param.requires_grad:\n",
    "                        #         l2_loss += torch.norm(param, p=2) ** 2\n",
    "                        # loss += lambda_l2 * l2_loss / 2\n",
    "\n",
    "                        loss.backward() #反向传播\n",
    "                        optimizer.step() #更新参数\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0) #损失累加 # 使用 inputs.size(0) 获取批次大小\n",
    "                running_corrects += torch.sum(preds == labels.data) #正确预测数量累加\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader[phase].dataset) #平均损失\n",
    "            epoch_acc = running_corrects.double() / len(dataloader[phase].dataset) #准确率\n",
    "            logging.info('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc.cpu().item())\n",
    "                        # 使用验证集上的损失更新学习率\n",
    "            if scheduler and phase == 'val':\n",
    "                scheduler.step(epoch_loss)\n",
    "    return model, history #返回模型和训练历史\n",
    "\n",
    "def plot_train_history(history):\n",
    "    \"\"\"\n",
    "    绘制训练和验证损失及准确率随时间的变化图\n",
    "    \"\"\"\n",
    "    epochs = range(len(history['train_loss']))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # 绘制损失变化\n",
    "    ax1.plot(epochs, history['train_loss'], label='Train Loss')\n",
    "    ax1.plot(epochs, history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend()\n",
    "\n",
    "    # 绘制准确率变化\n",
    "    ax2.plot(epochs, history['train_acc'], label='Train Accuracy')\n",
    "    ax2.plot(epochs, history['val_acc'], label='Validation Accuracy')\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "e0caef98af70245a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:29.315577Z",
     "start_time": "2025-01-23T13:58:25.547681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#载入数据\n",
    "data_file = './data/train_set.csv'\n",
    "data = pd.read_csv(data_file,sep='\\t')\n",
    "labels = data['label'].values\n",
    "texts = data['text'].values"
   ],
   "id": "a1a572a2e387bb06",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:29.320087Z",
     "start_time": "2025-01-23T13:58:29.317576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def text_to_int(text, vocab, max_len):\n",
    "    int_list = [vocab.get(word, vocab['<unk>']) for word in text.split()]\n",
    "    padded = int_list[:max_len]\n",
    "    padding_needed = max_len - len(padded)\n",
    "    if padding_needed > 0:\n",
    "        padded.extend([vocab['[PAD]']] * padding_needed)\n",
    "    return padded"
   ],
   "id": "52e55840024126c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:49.214095Z",
     "start_time": "2025-01-23T13:58:29.337914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#将文本转换为整数列表\n",
    "texts_as_int_lists = [text_to_int(text, vocab, max_len=3000) for text in texts]"
   ],
   "id": "7e3a8284af6713b4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T14:22:19.688962Z",
     "start_time": "2025-01-23T14:22:10.999481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#分割数据集为训练集和验证集\n",
    "X_train,X_test,y_train,y_test = train_test_split(texts_as_int_lists,labels,test_size=0.2,random_state=42)\n",
    "\n",
    "#创建数据集对象\n",
    "train_dataset = TextDataset(X_train,y_train,word_vectors,max_len=3000)\n",
    "test_dataset = TextDataset(X_test,y_test,word_vectors,max_len=3000)\n",
    "\n",
    "#创建数据加载器\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset,batch_size=64,shuffle=True),\n",
    "    'val': DataLoader(test_dataset,batch_size=64,shuffle=False)\n",
    "}"
   ],
   "id": "d2a05ee7cef25085",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T14:22:20.692932Z",
     "start_time": "2025-01-23T14:22:20.656224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 初始化模型、损失函数和优化器\n",
    "hidden_dim = 128  # LSTM 隐藏层维度\n",
    "output_dim = 14  # 类别数量\n",
    "model = BiLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_layers=2,  # 使用两层LSTM\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-5)"
   ],
   "id": "73234a23a4cf6021",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T14:40:25.736129Z",
     "start_time": "2025-01-23T14:40:24.722927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#开始训练模型\n",
    "trained_model, history = train_model(model,dataloaders,criterion,optimizer,num_epochs=15)"
   ],
   "id": "5c4ed4ee1901a1a8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 22:40:24,725 INFO: Epoch 1/15\n",
      "2025-01-23 22:40:24,725 INFO: ----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#开始训练模型\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m trained_model, history \u001B[38;5;241m=\u001B[39m train_model(model,dataloaders,criterion,optimizer,num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15\u001B[39m)\n",
      "Cell \u001B[1;32mIn[5], line 51\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, dataloader, criterion, optimizer, num_epochs, scheduler)\u001B[0m\n\u001B[0;32m     48\u001B[0m             loss\u001B[38;5;241m.\u001B[39mbackward() \u001B[38;5;66;03m#反向传播\u001B[39;00m\n\u001B[0;32m     49\u001B[0m             optimizer\u001B[38;5;241m.\u001B[39mstep() \u001B[38;5;66;03m#更新参数\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m inputs\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;66;03m#损失累加 # 使用 inputs.size(0) 获取批次大小\u001B[39;00m\n\u001B[0;32m     52\u001B[0m     running_corrects \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(preds \u001B[38;5;241m==\u001B[39m labels\u001B[38;5;241m.\u001B[39mdata) \u001B[38;5;66;03m#正确预测数量累加\u001B[39;00m\n\u001B[0;32m     54\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m running_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataloader[phase]\u001B[38;5;241m.\u001B[39mdataset) \u001B[38;5;66;03m#平均损失\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#保存模型\n",
    "# 保存模型参数到文件\n",
    "torch.save(trained_model.state_dict(), 'models/word2vec_RNN.pth')"
   ],
   "id": "1db4e0ed54b04e20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 设定设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载模型的状态字典\n",
    "# 首先创建一个与原模型架构相同的实例\n",
    "model = BiLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_layers=2,  # 使用两层LSTM\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "# 然后加载状态字典\n",
    "model.load_state_dict(torch.load('models/word2vec_RNN.pth', map_location=device,weights_only=True))\n",
    "model.to(device)  # 将模型移动到相应的设备上\n",
    "model.eval()  # 切换到评估模式"
   ],
   "id": "f562ed38226d8a5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#构造测试数据集\n",
    "test_data_file = './data/test_a.csv'\n",
    "test_data = pd.read_csv(test_data_file,sep='\\t')\n",
    "test_texts = test_data['text'].values\n",
    "test_texts_as_int_lists = [text_to_int(text, vocab, max_len=900) for text in test_texts]\n",
    "test_dataset = TextDataset(test_texts_as_int_lists, [0] * len(test_texts_as_int_lists), word_vectors, max_len=900)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "57d97e1a57b17c08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#预测新数据\n",
    "def predict(model, dataloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # 将模型移动到相应的设备\n",
    "    model.eval()      # 切换到评估模式\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:  # 假设新数据没有标签\n",
    "            inputs = inputs.to(device) # 将数据移动到相应的设备\n",
    "            outputs = model(inputs)\n",
    "            # 对于分类任务，通常会使用softmax函数将输出转换为概率分布，然后选择具有最高概率的类别作为预测结果。直接使用torch.max函数获取每行的最大值及其索引。只关心类别索引（预测标签），所以第二个返回值preds就是需要的结果\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # # 将预测结果从PyTorch张量转换为NumPy数组，并添加到predictions列表中。.cpu()方法确保即使原始张量位于GPU上，也可以安全地转换为NumPy格式。\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    return predictions"
   ],
   "id": "5697d0e80d34e7a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "predictions = predict(model, test_dataloader)",
   "id": "17e5e618e7cae51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pd.DataFrame(predictions,columns=['label']).to_csv('models/word2vec_textCNN_submit5.csv', index=False)",
   "id": "77e989d0798adcb5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
