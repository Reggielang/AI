{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-22T06:41:03.396711Z",
     "start_time": "2025-01-22T06:40:56.113319Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import logging\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T06:41:05.576214Z",
     "start_time": "2025-01-22T06:41:04.827649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 预训练的词向量文件路径\n",
    "vec_path = \"word2vec.txt\"  # 替换为实际路径\n",
    "# 加载词向量文件\n",
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(vec_path, binary=False)"
   ],
   "id": "f55657b3e4e49c4c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 14:41:04,829 INFO: loading projection weights from word2vec.txt\n",
      "2025-01-22 14:41:05,572 INFO: KeyedVectors lifecycle event {'msg': 'loaded (5971, 200) matrix of type float32 from word2vec.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-01-22T14:41:05.572649', 'gensim': '4.3.3', 'python': '3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:48:34) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T06:41:08.707154Z",
     "start_time": "2025-01-22T06:41:08.693842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 获取原始词向量的维度\n",
    "embedding_dim = word_vectors.vector_size\n",
    "\n",
    "# 初始化词汇表，包含特殊标记\n",
    "vocab = {'<unk>': 0, '[PAD]': 1}  # <unk> 和 [PAD] 的索引分别为 0 和 1\n",
    "\n",
    "# 将原始词向量添加到词汇表中\n",
    "for idx, word in enumerate(word_vectors.index_to_key, start=len(vocab)):\n",
    "    vocab[word] = idx\n",
    "\n",
    "# 更新 word_vectors，确保包含特殊标记\n",
    "special_tokens = {'<unk>': np.random.uniform(-0.25, 0.25, embedding_dim),\n",
    "                  '[PAD]': np.zeros(embedding_dim)}\n",
    "\n",
    "for token, vector in special_tokens.items():\n",
    "    if token not in word_vectors:\n",
    "        word_vectors.add_vector(token, vector)\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))"
   ],
   "id": "f29f862aa4870588",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T06:41:20.002810Z",
     "start_time": "2025-01-22T06:41:19.996539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#数据集处理\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word_vectors, max_len, unk_token='<unk>'):\n",
    "        \"\"\"\n",
    "        文本数据集类\n",
    "        :param texts: 文本数据列表，每个元素是一个词索引列表。\n",
    "        :param labels: 标签列表，与 texts 中的文本一一对应。\n",
    "        :param word_vectors: 词向量对象，包含词汇表和对应的词向量。\n",
    "        :param max_len: 每个文本的最大长度，用于填充或截断文本。\n",
    "        :param unk_token: 未知词标记，默认为 '<unk>'.\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.word_vectors = word_vectors\n",
    "        self.unk_token = unk_token\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # 确保 <unk> 和 [PAD] 已经存在于 word_vectors 中\n",
    "        assert '<unk>' in self.word_vectors.key_to_index, \"Vocabulary does not contain '<unk>' token.\"\n",
    "        assert '[PAD]' in self.word_vectors.key_to_index, \"Vocabulary does not contain '[PAD]' token.\"\n",
    "\n",
    "        # 填充或截断文本\n",
    "        self.text = [self.pad_text(text, max_len) for text in texts]\n",
    "\n",
    "    def pad_text(self, text, max_len):\n",
    "        \"\"\"\n",
    "        填充或截断文本以匹配指定的最大长度\n",
    "        :param text: 文本数据列表，每个元素是一个词索引列表。\n",
    "        :param max_len: 每个文本的最大长度，用于填充或截断文本。\n",
    "        \"\"\"\n",
    "        if not isinstance(text, list):\n",
    "            raise ValueError(\"Expected a list of integers as input.\")\n",
    "\n",
    "        padded = text[:max_len]  # 截取前 max_len 个元素\n",
    "        padding_needed = max_len - len(padded)\n",
    "        if padding_needed > 0:\n",
    "            padded.extend([self.word_vectors.key_to_index.get('[PAD]', 0)] * padding_needed)  # 使用[PAD]索引填充\n",
    "        return padded\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集中样本的数量\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        获取指定索引的数据\n",
    "        :param index: 数据索引。\n",
    "        :return: 一个元组，包含文本数据和对应的标签。\n",
    "        \"\"\"\n",
    "        text_tensor = torch.tensor(self.text[index], dtype=torch.long)  # 只传递词索引\n",
    "        label_tensor = torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        return text_tensor, label_tensor"
   ],
   "id": "6ab5511fc1191b2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T06:41:21.847654Z",
     "start_time": "2025-01-22T06:41:21.841983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,num_filters,filter_sizes,num_classes,dropout):\n",
    "        \"\"\"\n",
    "        文本分类模型\n",
    "        :param vocab_size: 词汇表大小。\n",
    "        :param embedding_dim: 嵌入维度。\n",
    "        :param num_filters: 卷积核数量。\n",
    "        :param filter_sizes: 卷积核大小列表。\n",
    "        :param num_classes: 分类类别数量。\n",
    "        :param dropout: 丢弃率。\n",
    "        \"\"\"\n",
    "        super(TextCNN,self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim) # 词嵌入层 #词嵌入层\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1,num_filters,(fs,embedding_dim)) for fs in filter_sizes]) #卷积层\n",
    "        self.dropout = nn.Dropout(dropout) #丢弃层\n",
    "        self.fc = nn.Linear(len(filter_sizes)*num_filters,num_classes) #全连接层\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        :param x: 输入数据，形状为 (batch_size, seq_len)。\n",
    "        :return: 输出数据，形状为 (batch_size, num_classes)。\n",
    "        \"\"\"\n",
    "        x = self.embedding(x).unsqueeze(1) #形状为 (batch_size, 1, seq_len, embedding_dim)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs] #relu激活函数，形状为 (batch_size, num_filters, seq_len - filter_size + 1)\n",
    "        x = [F.max_pool1d(i,i.size(2)).squeeze(2) for i in x] #最大池化，形状为 (batch_size, num_filters)\n",
    "        x = torch.cat(x, 1) #形状为 (batch_size, num_filters * len(filter_sizes))\n",
    "        x=self.dropout(x) #丢弃层\n",
    "        logits = self.fc(x) #全连接层\n",
    "        return logits"
   ],
   "id": "98a5c7a07006e06e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T06:44:50.764804Z",
     "start_time": "2025-01-22T06:44:50.756976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model,dataloader,criterion,optimizer,num_epochs=25):\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "    :param model: 模型。\n",
    "    :param dataloader: 数据加载器。\n",
    "    :param criterion: 损失函数。\n",
    "    :param optimizer: 优化器。\n",
    "    :param num_epochs: 训练轮数。\n",
    "    :return: 训练损失列表和准确率列表。\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logging.info('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        logging.info('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 训练模式\n",
    "            else:\n",
    "                model.eval()   # 评估模式\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad() #梯度清零\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'): #是否计算梯度\n",
    "                    outputs = model(inputs) #前向传播\n",
    "                    _, preds = torch.max(outputs, 1) #预测结果\n",
    "                    loss = criterion(outputs, labels) #损失函数\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward() #反向传播\n",
    "                        optimizer.step() #更新参数\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0) #损失累加 # 使用 inputs.size(0) 获取批次大小\n",
    "                running_corrects += torch.sum(preds == labels.data) #正确预测数量累加\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader[phase].dataset) #平均损失\n",
    "            epoch_acc = running_corrects.double() / len(dataloader[phase].dataset) #准确率\n",
    "            logging.info('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            history[f'{phase}_loss'].append(epoch_loss) #记录损失\n",
    "\n",
    "def plot_train_history(history):\n",
    "    \"\"\"\n",
    "    绘制训练和验证损失随时间的变化图\n",
    "    \"\"\"\n",
    "    epochs = range(len(history['train_loss']))\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(epochs, history['train_loss'],label='train_loss')\n",
    "    plt.plot(epochs, history['val_loss'], label='val_loss')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "4ef2ba096e6b08fc",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T06:41:32.267604Z",
     "start_time": "2025-01-22T06:41:25.419785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#载入数据\n",
    "data_file = '../data/train_set.csv'\n",
    "data = pd.read_csv(data_file,sep='\\t')\n",
    "labels = data['label'].values\n",
    "texts = data['text'].values\n",
    "#把字符串转换为整数列表\n",
    "# texts_as_int_lists = [list(map(int, text.split())) for text in texts]"
   ],
   "id": "ea7cb4b0e91b6be6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T06:42:13.181040Z",
     "start_time": "2025-01-22T06:41:43.088321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def text_to_int(text, vocab, max_len):\n",
    "    int_list = [vocab.get(word, vocab['<unk>']) for word in text.split()]\n",
    "    padded = int_list[:max_len]\n",
    "    padding_needed = max_len - len(padded)\n",
    "    if padding_needed > 0:\n",
    "        padded.extend([vocab['[PAD]']] * padding_needed)\n",
    "    return padded\n",
    "\n",
    "texts_as_int_lists = [text_to_int(text, vocab, max_len=900) for text in texts]"
   ],
   "id": "7506ba17c0b61e0a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T06:42:29.964629Z",
     "start_time": "2025-01-22T06:42:26.291786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#分割数据集为训练集和验证集\n",
    "X_train,X_test,y_train,y_test = train_test_split(texts_as_int_lists,labels,test_size=0.2,random_state=42)\n",
    "\n",
    "#创建数据集对象\n",
    "train_dataset = TextDataset(X_train,y_train,word_vectors,max_len=900)\n",
    "test_dataset = TextDataset(X_test,y_test,word_vectors,max_len=900)\n",
    "\n",
    "#创建数据加载器\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset,batch_size=64,shuffle=True),\n",
    "    'val': DataLoader(test_dataset,batch_size=64,shuffle=False)\n",
    "}"
   ],
   "id": "493fe9fb474dedb2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T06:42:34.396296Z",
     "start_time": "2025-01-22T06:42:32.697821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#初始化模型、损失函数和优化器\n",
    "model = TextCNN(vocab_size=len(vocab),\n",
    "                embedding_dim=embedding_dim,\n",
    "                num_filters=128,\n",
    "                filter_sizes=[3,4,5],\n",
    "                num_classes=14,\n",
    "                dropout=0.2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)"
   ],
   "id": "4cf206543465efb",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-22T06:44:58.216301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#开始训练模型\n",
    "trained_mode5l, history = train_model(model,dataloaders,criterion,optimizer,num_epochs=25)"
   ],
   "id": "f2c961724dc91e85",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 14:44:58,219 INFO: Epoch 1/25\n",
      "2025-01-22 14:44:58,220 INFO: ----------\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T23:10:22.067325Z",
     "start_time": "2025-01-21T23:10:22.065358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#可视化训练历史情况\n",
    "plot_train_history(history)"
   ],
   "id": "aa34ecdfe947a71b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:03:07.098886Z",
     "start_time": "2025-01-22T02:03:07.094887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#保存模型\n",
    "model.load_state_dict(torch.load('path_to_saved_model.pth'))\n",
    "model.eval()  # 切换到评估模式"
   ],
   "id": "22fcd2fcc660db7e",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:03:07.156851Z",
     "start_time": "2025-01-22T02:03:07.153095Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e276b69f7ba7d841",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:03:07.268805Z",
     "start_time": "2025-01-22T02:03:07.264291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#预测新数据\n",
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:  # 假设新数据没有标签\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    return predictions"
   ],
   "id": "be018c2829bcf5e2",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T03:15:17.761582Z",
     "start_time": "2025-01-22T03:15:17.756571Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 59,
   "source": [
    "texts = [\n",
    "    \"1001 1002 1003\",\n",
    "    \"1004 1005\",\n",
    "    \"1006\"\n",
    "]"
   ],
   "id": "72d1a90fbceb557d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d8227e6ad0d23f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
