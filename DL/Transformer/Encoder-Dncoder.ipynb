{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1.编码器-解码器结构",
   "id": "e9d588a87b72c420"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:15:08.947580Z",
     "start_time": "2025-02-10T02:15:06.593484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import datetime\n",
    "from DL.mnist_dnn import target"
   ],
   "id": "5c64d6184d6f2fb2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2e6fb581ca3def30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:15:10.256729Z",
     "start_time": "2025-02-10T02:15:10.253519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#编码器\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,dropout_p=0.1):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        self.hidden_size = hidden_size #隐藏层状态的维度\n",
    "        self.embedding = nn.Embedding(input_size,hidden_size) #嵌入层，用于将输入的索引序列转换为密集的向量表示\n",
    "        self.rnn= nn.RNN(input_size=hidden_size,hidden_size=hidden_size,num_layers=1,batch_first=True) #RNN循环神经网络层，用于处理序列数据\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def forward(self,input):\n",
    "        x = self.embedding(input) #将输入序列嵌入为密集的向量表示\n",
    "        x = self.dropout(x) #在嵌入向量上应用dropout进行随机失活\n",
    "        output, hidden = self.rnn(x) #通过RNN层处理嵌入向量，得到输出和最终隐藏状态\n",
    "        return output, hidden"
   ],
   "id": "97c0fbe6a9bc8325",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:15:46.280418Z",
     "start_time": "2025-02-10T02:15:46.275722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = EncoderRNN(input_size=10,hidden_size=5) #假设输入序列的词汇表大小为10，隐藏状态的维度为5\n",
    "input_seq = th.tensor([[0,1,2,3,4,5,6,7,8,9]]) #生成一个输入序列\n",
    "output, hidden = encoder(input_seq) #通过编码器进行前向传播\n",
    "print('输入向量的维度：',input_seq.size())\n",
    "print('输出向量的维度：',output.size())\n",
    "print('最终隐藏状态的维度：',hidden.size())"
   ],
   "id": "97dbeaf7987a43e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入向量的维度： torch.Size([1, 10])\n",
      "输出向量的维度： torch.Size([1, 10, 5])\n",
      "最终隐藏状态的维度： torch.Size([1, 1, 5])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:12:21.408990Z",
     "start_time": "2025-02-11T07:12:21.405818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2"
   ],
   "id": "8a64c56fbb3e25b3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:46:20.004374Z",
     "start_time": "2025-02-10T02:46:20.000317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#解码器\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size,output_size):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size,hidden_size) #嵌入层，用于将目标序列的索引转换为密集的向量表示\n",
    "        self.rnn = nn.RNN(hidden_size,hidden_size,num_layers=1,batch_first=True) #RNN循环神经网络层，用于处理序列数据\n",
    "        self.output = nn.Linear(hidden_size,output_size) #全连接层，用于将隐藏状态映射到输出序列的概率分布\n",
    "\n",
    "    def forward(self,encoder_outpus,encoder_hidden,target_tensor=None):\n",
    "        batch_size = encoder_outpus.size(0)\n",
    "        decoder_input = th.empty(batch_size,1,dtype=th.long).fill_(SOS_token) #创建一个初始的解码器输入，填充为起始标记 开始词元，表示开始生成一个序列\n",
    "        decoder_hidden = encoder_hidden #解码器的初始隐藏状态为编码器的最终隐藏状态\n",
    "        decoder_outputs = [] #用于存储解码器的输出序列\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output,decoder_hidden = self.forward_step(decoder_input,decoder_hidden) #对解码器进行一步前向传播\n",
    "            decoder_outputs.append(decoder_output) #将解码器的输出添加到输出序列中\n",
    "            if target_tensor is not None: #强制教学，使用目标序列的下一个标记作为解码器的输入\n",
    "                decoder_input = target_tensor[:,i].unsqueeze(1)\n",
    "            else: #没有目标序列，使用解码器的输出作为下一个输入\n",
    "                _,topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "        decoder_outputs = th.cat(decoder_outputs,dim=1) #将解码器的输出序列拼接起来\n",
    "        decoder_outputs = F.softmax(decoder_outputs,dim=-1) #对解码器的输出应用softmax函数，得到概率分布\n",
    "        return decoder_outputs,decoder_hidden,None #返回解码器的输出序列、最终隐藏状态和注意力权重\n",
    "    def forward_step(self,decoder_input,decoder_hidden):\n",
    "        x = self.embedding(decoder_input) #将解码器输入嵌入为密集的向量表示\n",
    "        x = F.relu(x) #通过ReLU激活函数对嵌入向量进行非线性变换\n",
    "        x,hidden = self.rnn(x,decoder_hidden) #通过RNN层处理嵌入向量，得到输出和最终隐藏状态\n",
    "        output = self.output(x)\n",
    "        return output,hidden #返回解码器的输出和最终隐藏状态"
   ],
   "id": "3a73c620b158c871",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:46:21.343206Z",
     "start_time": "2025-02-10T02:46:21.336664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoder = DecoderRNN(hidden_size=5,output_size=10) #假设隐藏状态的维度为5，输出序列的词汇表大小为10\n",
    "target_seq = th.tensor([[0,1,2,3,4,5,6,7,8,9]]) #生成一个目标序列\n",
    "encoder_outputs, encoder_hidden = encoder(input_seq) #通过编码器进行前向传播\n",
    "output, hidden,_ = decoder(encoder_outputs,encoder_hidden,input_seq)#通过解码器进行前向传播\n",
    "print('输出向量的维度：',output.size())"
   ],
   "id": "796cee5fd0cf9d78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出向量的维度： torch.Size([1, 10, 10])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:31:47.197402Z",
     "start_time": "2025-02-10T03:31:47.193977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Attention代码实现\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super(Attention,self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size,hidden_size) #线性变换矩阵，用于将编码器的输出映射到注意力权重\n",
    "        self.Ua = nn.Linear(hidden_size,hidden_size) #线性变换矩阵，用于将解码器的隐藏状态映射到注意力权重\n",
    "        self.Va = nn.Linear(hidden_size,1) #线性变换矩阵，用于将注意力权重映射到标量\n",
    "\n",
    "    def forward(self,query,keys):\n",
    "        scores = self.Va(th.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1) #将注意力分数的维度调整为(batch_size,1,seq_len)\n",
    "        weights = F.softmax(scores,dim=-1)  #对注意力分数应用softmax函数，得到注意力权重\n",
    "        context = th.bmm(weights,keys) #根据注意力权重加权计算上下文向量\n",
    "        return context,weights"
   ],
   "id": "93df84346e419c3",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:33:18.271827Z",
     "start_time": "2025-02-10T03:33:18.265586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#带注意力的解码器\n",
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size,output_size,dropout_p=0.1):\n",
    "        super(AttentionDecoderRNN,self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size,hidden_size) #嵌入层，用于将目标序列的索引转换为密集的向量表示\n",
    "        self.attention = Attention(hidden_size) #注意力机制\n",
    "        self.rnn = nn.RNN(2*hidden_size,hidden_size,batch_first=True)  #2*hidden_size表示输入特征的维度，hidden_size表示隐藏状态的维度\n",
    "        self.output = nn.Linear(hidden_size,output_size) #全连接层，用于将隐藏状态映射到输出序列的概率分布\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def forward(self,encoder_outputs,encoder_hidden,target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = th.empty(batch_size,1,dtype=th.long).fill_(SOS_token) #创建一个初始的解码器输入，填充为起始标记 开始词元，表示开始生成一个序列\n",
    "        decoder_hidden = encoder_hidden #解码器的初始隐藏状态为编码器的最终隐藏状态\n",
    "        decoder_outputs = [] #用于存储解码器的输出序列\n",
    "        attentions = [] #用于存储注意力权重\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output,decoder_hidden,attn_weights = self.forward_step(decoder_input,decoder_hidden,encoder_outputs) #对解码器进行一步前向传播\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights) #将注意力权重添加到注意力权重序列中\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:,i].unsqueeze(1)\n",
    "            else:\n",
    "                _,topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "        decoder_outputs = th.cat(decoder_outputs,dim=1) #将解码器的输出序列拼接起来\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs,dim=-1) #对解吗器的输出应用softmax函数，得到概率分布\n",
    "        attentions = th.cat(attentions,dim=1) #将注意力权重序列拼接起来\n",
    "        return decoder_outputs,decoder_hidden,attentions #返回解码器的输出序列、最终隐藏状态和注意力权重\n",
    "\n",
    "    def forward_step(self,input,hidden,encoder_outputs):\n",
    "        embedded = self.embedding(input) #将解码器输入嵌入为密集的向量表示\n",
    "        embedded = self.dropout(embedded) #在嵌入向量上应用dropout进行随机失活\n",
    "        query = hidden.permute(1,0,2) #将解码器的隐藏状态进行转置，用于与编码器的输出进行注意力计算\n",
    "        context,attn_weights = self.attention(query,encoder_outputs) #通过注意力机制计算上下文向量和注意力权重\n",
    "        rnn_input = th.cat((embedded,context),dim=2) #将嵌入向量和上下文向量进行拼接，作为RNN的输入\n",
    "        output,hidden = self.rnn(rnn_input,hidden) #通过RNN层处理拼接后的输入，得到输出和最终隐藏状态\n",
    "        output = self.output(output) #通过全连接层将RNN的输出映射到输出序列的概率分布\n",
    "        return output,hidden,attn_weights #返回解码器的输出、最终隐藏状态和注意力权重\n"
   ],
   "id": "1b2c632999d830a5",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:33:19.298179Z",
     "start_time": "2025-02-10T03:33:19.292872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoder = AttentionDecoderRNN(hidden_size=5,output_size=10)\n",
    "target_seq = th.tensor([[0,1,2,3,4,5,6,7,8,9]])\n",
    "encoder_outputs, encoder_hidden = encoder(input_seq)\n",
    "output, hidden,attn_weights = decoder(encoder_outputs,encoder_hidden,input_seq) #通过解码器进行前向传播\n",
    "print('输出向量的维度：',output.size())\n",
    "print('注意力权重的维度：',attn_weights.size())"
   ],
   "id": "b4902462d4e0ee43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出向量的维度： torch.Size([1, 10, 10])\n",
      "注意力权重的维度： torch.Size([1, 10, 10])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:11:42.171873Z",
     "start_time": "2025-02-11T07:11:42.166497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "\n",
    "#实战：日期转换 将中文的‘年-月-日’格式的日期转换为英文格式‘day/month/year’ 区间是1950-2050年 增加难度，中文提供‘YY-MM-DD’ 缺少前面2位，模型需要推测出前面的2位数字\n",
    "class DateDataset(Dataset):\n",
    "    def __init__(self, n):\n",
    "        # 初始化两个空列表，用于存储中文和英文日期\n",
    "        self.date_cn = []\n",
    "        self.date_en = []\n",
    "        for _ in range(n):\n",
    "            # 随机生成年份、月份和日期\n",
    "            year = random.randint(1950, 2050)\n",
    "            month = random.randint(1, 12)\n",
    "            day = random.randint(1, 28)  # 假设最大日期为28日\n",
    "            date = datetime.date(year, month, day)\n",
    "            # 格式化日期并添加到对应的列表中\n",
    "            self.date_cn.append(date.strftime(\"%y-%m-%d\"))\n",
    "            self.date_en.append(date.strftime(\"%d/%b/%Y\"))\n",
    "        # 创建一个词汇集，包含0-9的数字、\"-\"、\"/\"和英文日期中的月份缩写\n",
    "        self.vocab = set([str(i) for i in range(0, 10)] +\n",
    "                         [\"-\", \"/\"] + [i.split(\"/\")[1] for i in self.date_en])\n",
    "        # 创建一个词汇到索引的映射，其中\"<SOS>\"、\"<EOS>\"和\"<PAD>\"分别对应开始、结束和填充标记\n",
    "        self.word2index = {v: i for i, v in enumerate(sorted(list(self.vocab)), start=3)}\n",
    "        self.word2index[\"<PAD>\"] = PAD_token\n",
    "        self.word2index[\"<SOS>\"] = SOS_token\n",
    "        self.word2index[\"<EOS>\"] = EOS_token\n",
    "        # 将开始、结束和填充标记添加到词汇集中\n",
    "        self.vocab.add(\"<SOS>\")\n",
    "        self.vocab.add(\"<EOS>\")\n",
    "        self.vocab.add(\"<PAD>\")\n",
    "        # 创建一个索引到词汇的映射\n",
    "        self.index2word = {i: v for v, i in self.word2index.items()}\n",
    "        # 初始化输入和目标列表\n",
    "        self.input, self.target = [], []\n",
    "        for cn, en in zip(self.date_cn, self.date_en):\n",
    "            # 将日期字符串转换为词汇索引列表，然后添加到输入和目标列表中\n",
    "            self.input.append([self.word2index[v] for v in cn])\n",
    "            self.target.append(\n",
    "                [self.word2index[\"<SOS>\"], ] +\n",
    "                [self.word2index[v] for v in en[:3]] +\n",
    "                [self.word2index[en[3:6]]] +\n",
    "                [self.word2index[v] for v in en[6:]] +\n",
    "                [self.word2index[\"<EOS>\"], ]\n",
    "            )\n",
    "        # 将输入和目标列表转换为NumPy数组\n",
    "        self.input, self.target = np.array(self.input), np.array(self.target)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据集的长度，即输入的数量\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 返回给定索引的输入、目标和目标的长度\n",
    "        return self.input[index], self.target[index], len(self.target[index])\n",
    "\n",
    "    @property\n",
    "    def num_word(self):\n",
    "        # 返回词汇表的大小\n",
    "        return len(self.vocab)"
   ],
   "id": "8f594531020b2a7b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:12:28.566590Z",
     "start_time": "2025-02-11T07:12:28.557032Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = DateDataset(1000)",
   "id": "79444eb40b17fe7b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:12:30.966629Z",
     "start_time": "2025-02-11T07:12:30.962205Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.date_cn[:5], dataset.date_en[:5]",
   "id": "f77919d02f4b12d3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['29-07-07', '41-06-27', '66-12-01', '37-08-17', '29-10-25'],\n",
       " ['07/Jul/2029', '27/Jun/2041', '01/Dec/1966', '17/Aug/2037', '25/Oct/2029'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:12:37.131166Z",
     "start_time": "2025-02-11T07:12:37.127827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "MAX_LENGTH = 11\n",
    "hidden_size = 128\n",
    "learning_rate = 0.001"
   ],
   "id": "f8fa301775942e87",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:11:57.686921Z",
     "start_time": "2025-02-11T07:11:57.675218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "encoder = EncoderRNN(input_size=dataset.num_word, hidden_size=hidden_size)\n",
    "decoder = AttentionDecoderRNN(hidden_size=hidden_size, output_size=dataset.num_word)\n",
    "encoder_optimizer = th.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = th.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ],
   "id": "98fc055d8c75cde6",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,drop_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      2\u001B[0m encoder \u001B[38;5;241m=\u001B[39m EncoderRNN(input_size\u001B[38;5;241m=\u001B[39mdataset\u001B[38;5;241m.\u001B[39mnum_word, hidden_size\u001B[38;5;241m=\u001B[39mhidden_size)\n\u001B[0;32m      3\u001B[0m decoder \u001B[38;5;241m=\u001B[39m AttentionDecoderRNN(hidden_size\u001B[38;5;241m=\u001B[39mhidden_size, output_size\u001B[38;5;241m=\u001B[39mdataset\u001B[38;5;241m.\u001B[39mnum_word)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:56:55.502127Z",
     "start_time": "2025-02-10T03:56:03.450462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(n_epochs+1):\n",
    "    total_loss = 0\n",
    "    for input_seq, target_seq,target_length in dataloader:\n",
    "        encoder_optimizer.zero_grad() #梯度清零\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_outputs, encoder_hidden = encoder(input_seq) #通过编码器进行前向传播\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs,encoder_hidden,target_seq) #通过解码器进行前向传播\n",
    "        loss = criterion(decoder_outputs.view(-1, decoder_outputs.size(-1)), target_seq.view(-1).long()) #计算损失\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step() #更新编码器参数\n",
    "        decoder_optimizer.step() #更新解码器参数\n",
    "        total_loss += loss.item() #累加损失\n",
    "\n",
    "    total_loss = total_loss/len(dataloader)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch [{i}/{n_epochs}], Loss: {total_loss:.4f}')"
   ],
   "id": "97bfc5858dcef1bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 1.9712\n",
      "Epoch [10/100], Loss: 0.0116\n",
      "Epoch [20/100], Loss: 0.0035\n",
      "Epoch [30/100], Loss: 0.0022\n",
      "Epoch [40/100], Loss: 0.0010\n",
      "Epoch [50/100], Loss: 0.0005\n",
      "Epoch [60/100], Loss: 0.0004\n",
      "Epoch [70/100], Loss: 0.0003\n",
      "Epoch [80/100], Loss: 0.0002\n",
      "Epoch [90/100], Loss: 0.0002\n",
      "Epoch [100/100], Loss: 0.0025\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T06:23:05.298039Z",
     "start_time": "2025-02-10T06:23:05.273150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#评估模式\n",
    "def evaluate(encoder, decoder,x):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    encoder_outputs, encoder_hidden = encoder(th.tensor(np.array([x])))\n",
    "    start = th.ones(x.shape[0],1) #创建一个初始的解码器输入，填充为起始标记 开始词元，表示开始生成一个序列\n",
    "    start[:,0] = th.tensor([SOS_token]).long()\n",
    "    decoder_outputs, _, _ = decoder(encoder_outputs,encoder_hidden)\n",
    "    _, topi = decoder_outputs.topk(1)\n",
    "    decoded_ids = topi.squeeze()\n",
    "    decoded_words =[]\n",
    "    for idx in decoded_ids:\n",
    "        decoded_words.append(dataset.index2word[idx.item()])\n",
    "    return \"\".join(decoded_words)\n",
    "\n",
    "for i in range(5):\n",
    "    predict = evaluate(encoder, decoder, dataset[i][0])\n",
    "    print(f\"input:{dataset.date_cn[i]}, target:{dataset.date_en[i]}, prediction:{predict}\")\n"
   ],
   "id": "a5c4aed1f5f40687",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:14-09-16, target:16/Sep/2014, prediction:<SOS>16/Sep/2014<EOS>\n",
      "input:65-05-09, target:09/May/1965, prediction:<SOS>09/May/1965<EOS>\n",
      "input:58-09-10, target:10/Sep/1958, prediction:<SOS>10/Sep/1958<EOS>\n",
      "input:91-11-26, target:26/Nov/1991, prediction:<SOS>26/Nov/1991<EOS>\n",
      "input:59-11-04, target:04/Nov/1959, prediction:<SOS>04/Nov/1959<EOS>\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Transformer架构",
   "id": "51407d25bfc3c63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:03:18.943598Z",
     "start_time": "2025-02-11T07:03:18.941564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn.functional import cross_entropy, softmax, relu\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ],
   "id": "f7a03349f8082b4f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:03:22.342575Z",
     "start_time": "2025-02-11T07:03:22.336533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, model_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        # 每个注意力头的维度\n",
    "        self.head_dim = model_dim // n_head\n",
    "        # 注意力头的数量\n",
    "        self.n_head = n_head\n",
    "        # 模型的维度\n",
    "        self.model_dim = model_dim\n",
    "        # 初始化线性变换层，用于生成query、key和value\n",
    "        self.wq = nn.Linear(model_dim, n_head * self.head_dim)\n",
    "        self.wk = nn.Linear(model_dim, n_head * self.head_dim)\n",
    "        self.wv = nn.Linear(model_dim, n_head * self.head_dim)\n",
    "        # 输出的全连接层\n",
    "        self.output_dense = nn.Linear(model_dim, model_dim)\n",
    "        # Dropout层，用于防止模型过拟合\n",
    "        self.output_drop = nn.Dropout(drop_rate)\n",
    "        # 层归一化，用于稳定神经网络的训练\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "        self.attention = None\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # 保存原始输入q，用于后续的残差连接\n",
    "        residual = q\n",
    "        # 分别对输入q,k,v做线性变换，生成query、key和value\n",
    "        query = self.wq(q)\n",
    "        key   = self.wk(k)\n",
    "        value = self.wv(v)\n",
    "        # 对生成的query、key和value进行头分割，以便进行多头注意力计算\n",
    "        query = self.split_heads(query)\n",
    "        key   = self.split_heads(key)\n",
    "        value = self.split_heads(value)\n",
    "        # 计算上下文向量\n",
    "        context = self.scaled_dot_product_attention(query, key, value, mask)\n",
    "        # 对上下文向量进行线性变换\n",
    "        output = self.output_dense(context)\n",
    "        # 添加dropout\n",
    "        output = self.output_drop(output)\n",
    "        # 添加残差连接并进行层归一化\n",
    "        output = self.layer_norm(residual + output)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # 将输入x的shape变为(n, step, n_head, head_dim)，然后进行重排，得到(n, n_head, step, head_dim)\n",
    "        x = th.reshape(x, (x.shape[0], x.shape[1], self.n_head, self.head_dim))\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    #缩放点积注意力是一种注意力机制，用于计算序列中不同位置之间的相关性。缩放的目的是为了避免在计算注意力分数时出现数值不稳定的情况。从而导致在后续训练中的梯度消失或梯度爆炸的问题。\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        # 计算缩放因子\n",
    "        dk = th.tensor(k.shape[-1]).type(th.float)\n",
    "        # 计算注意力分数\n",
    "        score = th.matmul(q, k.permute(0, 1, 3, 2)) / (th.sqrt(dk) + 1e-8)\n",
    "        if mask is not None:\n",
    "            # 如果提供了mask，将mask位置的分数设置为负无穷，使得这些位置的softmax值接近0\n",
    "            score = score.masked_fill_(mask,-np.inf)\n",
    "        # 计算softmax得到注意力权重\n",
    "        self.attention = softmax(score,dim=-1)\n",
    "        # 计算上下文向量\n",
    "        context = th.matmul(self.attention,v)\n",
    "        # 重排上下文向量的维度并进行维度合并\n",
    "        context = context.permute(0, 2, 1, 3)\n",
    "        context = context.reshape((context.shape[0], context.shape[1],-1))\n",
    "        return context"
   ],
   "id": "34f9316ae756d7fb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:05:34.375647Z",
     "start_time": "2025-02-11T07:05:34.372425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, model_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        # 前馈神经网络的隐藏层维度，设为模型维度的4倍\n",
    "        ffn_dim = model_dim * 4\n",
    "        # 第一个线性变换层，其输出维度为前馈神经网络的隐藏层维度\n",
    "        self.linear1 = nn.Linear(model_dim, ffn_dim)\n",
    "        # 第二个线性变换层，其输出维度为模型的维度\n",
    "        self.linear2 = nn.Linear(ffn_dim, model_dim)\n",
    "        # Dropout层，用于防止模型过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 层归一化，用于稳定神经网络的训练\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对输入x进行前馈神经网络的计算\n",
    "        # 首先，通过第一个线性变换层并使用relu作为激活函数\n",
    "        output = relu(self.linear1(x))\n",
    "        # 然后，通过第二个线性变换层\n",
    "        output = self.linear2(output)\n",
    "        # 接着，对输出做dropout\n",
    "        output = self.dropout(output)\n",
    "        # 最后，对输入x和前馈神经网络的输出做残差连接，然后进行层归一化\n",
    "        output = self.layer_norm(x + output)\n",
    "        return output  # 返回结果，其shape为[n, step, dim]"
   ],
   "id": "2bf6a7c381ed16c4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:07:43.627576Z",
     "start_time": "2025-02-11T07:07:43.623090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_head, emb_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        # 多头注意力机制层\n",
    "        self.mha = MultiHeadAttention(n_head, emb_dim, drop_rate)\n",
    "        # 前馈神经网络层\n",
    "        self.ffn = PositionWiseFFN(emb_dim, drop_rate)\n",
    "\n",
    "    def forward(self, xz, mask):\n",
    "        # xz的形状为 [n, step, emb_dim]\n",
    "        # 通过多头注意力机制层处理xz，得到context，其形状也为 [n, step, emb_dim]\n",
    "        context = self.mha(xz, xz, xz, mask)\n",
    "        # 将context传入前馈神经网络层，得到输出\n",
    "        output = self.ffn(context)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_head, emb_dim, drop_rate, n_layer):\n",
    "        super().__init__()\n",
    "        # 定义n_layer个EncoderLayer，保存在一个ModuleList中\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(n_head, emb_dim, drop_rate) for _ in range(n_layer)]\n",
    "        )\n",
    "\n",
    "    def forward(self, xz, mask):\n",
    "        # 依次通过所有的EncoderLayer\n",
    "        for encoder in self.encoder_layers:\n",
    "            xz = encoder(xz, mask)\n",
    "        return xz  # 返回的xz形状为 [n, step, emb_dim]"
   ],
   "id": "8a3b577c9de1442d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:07:53.085172Z",
     "start_time": "2025-02-11T07:07:53.081023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, n_head, model_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        # 定义两个多头注意力机制层\n",
    "        self.mha = nn.ModuleList([MultiHeadAttention(n_head, model_dim, drop_rate) for _ in range(2)])\n",
    "        # 定义一个前馈神经网络层\n",
    "        self.ffn = PositionWiseFFN(model_dim, drop_rate)\n",
    "\n",
    "    def forward(self, yz, xz, yz_look_ahead_mask, xz_pad_mask):\n",
    "        # 第一个注意力层的计算，三个输入均为yz，使用自注意力机制\n",
    "        dec_output = self.mha[0](yz, yz, yz, yz_look_ahead_mask)  # [n, step, model_dim]\n",
    "        # 第二个注意力层的计算，其中q来自前一个注意力层的输出，K和V来自编码器的输出\n",
    "        dec_output = self.mha[1](dec_output, xz, xz, xz_pad_mask)  # [n, step, model_dim]\n",
    "        # 通过前馈神经网络层\n",
    "        dec_output = self.ffn(dec_output)   # [n, step, model_dim]\n",
    "        return dec_output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_head, model_dim, drop_rate, n_layer):\n",
    "        super().__init__()\n",
    "        # 定义n_layer个DecoderLayer，保存在一个ModuleList中\n",
    "        self.num_layers = n_layer\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(n_head, model_dim, drop_rate) for _ in range(n_layer)]\n",
    "        )\n",
    "\n",
    "    def forward(self, yz, xz, yz_look_ahead_mask, xz_pad_mask):\n",
    "        # 依次通过所有的DecoderLayer\n",
    "        for decoder in self.decoder_layers:\n",
    "            yz = decoder(yz, xz, yz_look_ahead_mask, xz_pad_mask)\n",
    "        return yz  # 返回的yz形状为 [n, step, model_dim]"
   ],
   "id": "f729a355ffbc9742",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:08:02.983587Z",
     "start_time": "2025-02-11T07:08:02.979073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, emb_dim, n_vocab):\n",
    "        super().__init__()\n",
    "        # 生成位置编码矩阵\n",
    "        pos = np.expand_dims(np.arange(max_len), 1)  # [max_len, 1]\n",
    "        # 使用正弦和余弦函数生成位置编码\n",
    "        pe = pos / np.power(1000, 2*np.expand_dims(np.arange(emb_dim)//2, 0)/emb_dim)\n",
    "        pe[:, 0::2] = np.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = np.cos(pe[:, 1::2])\n",
    "        pe = np.expand_dims(pe, 0)  # [1, max_len, emb_dim]\n",
    "        self.pe = th.from_numpy(pe).type(th.float32)\n",
    "\n",
    "        # 定义词嵌入层\n",
    "        self.embeddings = nn.Embedding(n_vocab, emb_dim)\n",
    "        # 初始化词嵌入层的权重\n",
    "        self.embeddings.weight.data.normal_(0, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 确保位置编码在与词嵌入权重相同的设备上\n",
    "        device = self.embeddings.weight.device\n",
    "        self.pe = self.pe.to(device)\n",
    "        # 计算输入的词嵌入，并加上位置编码\n",
    "        x_embed = self.embeddings(x) + self.pe  # [n, step, emb_dim]\n",
    "        return x_embed  # [n, step, emb_dim]"
   ],
   "id": "2e401f254b3f347e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:08:29.961697Z",
     "start_time": "2025-02-11T07:08:29.958517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_zero(seqs, max_len):\n",
    "    # 初始化一个全是填充标识符PAD_token的二维矩阵，大小为(len(seqs), max_len)\n",
    "    padded = np.full((len(seqs), max_len), fill_value=PAD_token, dtype=np.int32)\n",
    "    for i, seq in enumerate(seqs):\n",
    "        # 将seqs中的每个序列seq的元素填入padded对应的行中，未填满的部分仍为PAD_token\n",
    "        padded[i, :len(seq)] = seq\n",
    "    return padded"
   ],
   "id": "2130c457537c2f4d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:10:41.312901Z",
     "start_time": "2025-02-11T07:10:41.306898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_vocab, max_len, n_layer=6, emb_dim=512, n_head=8, drop_rate=0.1, padding_idx=0):\n",
    "        super().__init__()\n",
    "        # 初始化最大长度、填充索引、词汇表大小\n",
    "        self.max_len = max_len\n",
    "        self.padding_idx = th.tensor(padding_idx)\n",
    "        self.dec_v_emb = n_vocab\n",
    "        # 初始化位置嵌入、编码器、解码器和输出层\n",
    "        self.embed = PositionEmbedding(max_len, emb_dim, n_vocab)\n",
    "        self.encoder = Encoder(n_head, emb_dim, drop_rate, n_layer)\n",
    "        self.decoder = Decoder(n_head, emb_dim, drop_rate, n_layer)\n",
    "        self.output = nn.Linear(emb_dim, n_vocab)\n",
    "        # 初始化优化器\n",
    "        self.opt = th.optim.Adam(self.parameters(), lr=0.002)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # 对输入和目标进行嵌入\n",
    "        x_embed, y_embed = self.embed(x), self.embed(y)\n",
    "        # 创建填充掩码\n",
    "        pad_mask = self._pad_mask(x)\n",
    "        # 对输入进行编码\n",
    "        encoded_z = self.encoder(x_embed, pad_mask)\n",
    "        # 创建前瞻掩码\n",
    "        yz_look_ahead_mask = self._look_ahead_mask(y)\n",
    "        # 将编码后的输入和前瞻掩码传入解码器\n",
    "        decoded_z = self.decoder(\n",
    "            y_embed, encoded_z, yz_look_ahead_mask, pad_mask)\n",
    "        # 通过输出层得到最终输出\n",
    "        output = self.output(decoded_z)\n",
    "        return output\n",
    "\n",
    "    def step(self, x, y):\n",
    "        # 清空梯度\n",
    "        self.opt.zero_grad()\n",
    "        # 计算输出和损失\n",
    "        logits = self(x, y[:, :-1])\n",
    "        loss = cross_entropy(logits.reshape(-1, self.dec_v_emb), y[:, 1:].reshape(-1))\n",
    "        # 进行反向传播\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        self.opt.step()\n",
    "        return loss.cpu().data.numpy(), logits\n",
    "\n",
    "    def _pad_bool(self, seqs):\n",
    "        # 创建掩码，标记哪些位置是填充的\n",
    "        return th.eq(seqs, self.padding_idx)\n",
    "\n",
    "    def _pad_mask(self, seqs):\n",
    "        # 将填充掩码扩展到合适的维度\n",
    "        len_q = seqs.size(1)\n",
    "        mask = self._pad_bool(seqs).unsqueeze(1).expand(-1, len_q, -1)\n",
    "        return mask.unsqueeze(1)\n",
    "\n",
    "    def _look_ahead_mask(self, seqs):\n",
    "        # 创建前瞻掩码，防止在生成序列时看到未来的信息\n",
    "        device = next(self.parameters()).device\n",
    "        _, seq_len = seqs.shape\n",
    "        mask = th.triu(th.ones((seq_len, seq_len), dtype=th.long),\n",
    "                       diagonal=1).to(device)\n",
    "        mask = th.where(self._pad_bool(seqs)[:, None, None, :], 1, mask[None, None, :, :]).to(device)\n",
    "        return mask > 0"
   ],
   "id": "b7f398c04693c57f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:12:51.274233Z",
     "start_time": "2025-02-11T07:12:46.572992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 初始化一个Transformer模型，设置词汇表大小、最大序列长度、层数、嵌入维度、多头注意力的头数、 dropout比率和填充标记的索引\n",
    "model = Transformer(n_vocab=dataset.num_word, max_len=MAX_LENGTH, n_layer=3, emb_dim=32, n_head=8, drop_rate=0.1, padding_idx=0)\n",
    "# 检测是否有可用的GPU，如果有，则使用GPU进行计算；如果没有，则使用CPU\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "# 将模型移动到相应的设备（CPU或GPU）\n",
    "model = model.to(device)\n",
    "# 创建一个数据集，包含1000个样本\n",
    "dataset = DateDataset(1000)\n",
    "# 创建一个数据加载器，设定批次大小为32，每个批次的数据会被打乱\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# 进行10个训练周期\n",
    "for i in range(10):\n",
    "    # 对于数据加载器中的每批数据\n",
    "    for input_tensor, target_tensor, _ in dataloader:\n",
    "        # 对输入和目标张量进行零填充，使其长度达到最大长度，然后将其转换为PyTorch张量，并移动到相应的设备（CPU或GPU）\n",
    "        input_tensor = th.from_numpy(\n",
    "            pad_zero(input_tensor, max_len=MAX_LENGTH)).long().to(device)\n",
    "        target_tensor = th.from_numpy(\n",
    "            pad_zero(target_tensor, MAX_LENGTH+1)).long().to(device)\n",
    "        # 使用模型的step方法进行一步训练，并获取损失值\n",
    "        loss, _ = model.step(input_tensor, target_tensor)\n",
    "    # 打印每个训练周期后的损失值\n",
    "    print(f\"epoch: {i+1}, \\tloss: {loss}\")"
   ],
   "id": "697e4f1025676c0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, \tloss: 1.4655067920684814\n",
      "epoch: 2, \tloss: 1.0785928964614868\n",
      "epoch: 3, \tloss: 0.9121214151382446\n",
      "epoch: 4, \tloss: 0.6275089383125305\n",
      "epoch: 5, \tloss: 0.34216246008872986\n",
      "epoch: 6, \tloss: 0.24774736166000366\n",
      "epoch: 7, \tloss: 0.15374785661697388\n",
      "epoch: 8, \tloss: 0.06169423833489418\n",
      "epoch: 9, \tloss: 0.03446295112371445\n",
      "epoch: 10, \tloss: 0.017239874228835106\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:13:04.868786Z",
     "start_time": "2025-02-11T07:13:04.865457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, x, y):\n",
    "    model.eval()\n",
    "    x = th.from_numpy(pad_zero([x], max_len=MAX_LENGTH)).long().to(device)\n",
    "    y = th.from_numpy(pad_zero([y], max_len=MAX_LENGTH)).long().to(device)\n",
    "    decoder_outputs = model(x, y)\n",
    "    _, topi = decoder_outputs.topk(1)\n",
    "    decoded_ids = topi.squeeze()\n",
    "    decoded_words = []\n",
    "    for idx in decoded_ids:\n",
    "        decoded_words.append(dataset.index2word[idx.item()])\n",
    "    return ''.join(decoded_words)"
   ],
   "id": "cea315ce0f918e14",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:13:18.743557Z",
     "start_time": "2025-02-11T07:13:18.685092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(5):\n",
    "    predict = evaluate(model, dataset[i][0], dataset[i][1])\n",
    "    print(f\"input: {dataset.date_cn[i]}, target: {dataset.date_en[i]}, predict: {predict}\")"
   ],
   "id": "46bd2bc83013cef8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 56-09-06, target: 06/Sep/1956, predict: 06/Sep/1956<EOS><PAD>\n",
      "input: 09-07-22, target: 22/Jul/2009, predict: 22/Jul/2009<EOS><PAD>\n",
      "input: 66-01-23, target: 23/Jan/1966, predict: 23/Jan/1966<EOS><PAD>\n",
      "input: 92-07-22, target: 22/Jul/1992, predict: 22/Jul/1992<EOS><PAD>\n",
      "input: 58-07-12, target: 12/Jul/1958, predict: 12/Jul/1958<EOS><PAD>\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:13:08.782426Z",
     "start_time": "2025-02-11T07:13:08.780428Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "54b8d752ce835ab3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:13:08.976177Z",
     "start_time": "2025-02-11T07:13:08.974343Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "400d50eabf16dbaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:13:09.121867Z",
     "start_time": "2025-02-11T07:13:09.119927Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5e5cd73e8ced5622",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T07:13:09.291994Z",
     "start_time": "2025-02-11T07:13:09.290268Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8d175b1db127c4cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9ebec720d3d69c2a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
