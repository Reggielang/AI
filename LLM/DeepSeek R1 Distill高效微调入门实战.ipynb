{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3c2264-ad51-4d26-ad78-17f1a61148c7",
   "metadata": {},
   "source": "### ä¸€ã€unslothå¿«é€Ÿä½¿ç”¨å…¥é—¨"
  },
  {
   "cell_type": "markdown",
   "id": "2a790496-d887-4d14-bf36-aeb2a4b0a944",
   "metadata": {},
   "source": "#### 1.å€ŸåŠ©unslothè¿›è¡Œæ¨¡å‹æ¨ç†"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "21465c7368f8a950"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:52:19.605443Z",
     "start_time": "2025-02-17T08:52:19.603412Z"
    }
   },
   "cell_type": "code",
   "source": "# modelscope download --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --local_dir DeepSeek-R1-Distill-Qwen-7B\n# modelscope download --model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --local_dir DeepSeek-R1-Distill-Qwen-1.5B",
   "id": "7fdbfdc699f8d43b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "#\n",
    "# # ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°\n",
    "# snapshot_download(repo_id=\"unsloth/DeepSeek-R1-Distill-Qwen-1.5B\", local_dir=\"unsloth-DeepSeek-R1-Distill-Qwen-1.5B\")"
   ],
   "id": "57c69c88fdd4bd9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3bcd26a2-92f4-477c-8db5-f330add5d33b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:32:13.176723Z",
     "start_time": "2025-02-18T08:32:06.354889Z"
    }
   },
   "source": "from unsloth import FastLanguageModel\n# import os\n# os.environ['CUDA_PATH'] = r'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.4'",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "e847f228-7910-43d6-91c6-515b9d9e2ef3",
   "metadata": {},
   "source": "}- å°è¯•ç”¨unslothè¿›è¡ŒLLamaæ¨¡å‹æ¨ç†"
  },
  {
   "cell_type": "markdown",
   "id": "f37319bd-28b9-4c00-9c24-bd7ed75cd922",
   "metadata": {},
   "source": "&emsp;&emsp;é¦–å…ˆè®¾ç½®å…³é”®å‚æ•°ï¼Œå¹¶è¯»å–æ¨¡å‹ï¼š"
  },
  {
   "cell_type": "code",
   "id": "e4608195-1e9d-47c6-8c50-e752da195fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:32:31.376208Z",
     "start_time": "2025-02-18T08:32:31.373814Z"
    }
   },
   "source": "max_seq_length = 2048 \ndtype = None \nload_in_4bit = False\n# load_in_4bit = True",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "620c826a-4fe6-4ea2-a67d-9b78b7a9425e",
   "metadata": {},
   "source": "> æ³¨ï¼Œè‹¥æ˜¾å­˜ä¸è¶³ï¼Œåˆ™å¯ä»¥load_in_4bit = Trueï¼Œè¿è¡Œ4 bité‡åŒ–ç‰ˆã€‚"
  },
  {
   "cell_type": "code",
   "id": "825bc87f-731e-4ccc-8b75-2815a0ea1038",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:32:49.289738Z",
     "start_time": "2025-02-18T08:32:33.430813Z"
    }
   },
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"../DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Qwen2 patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4080 SUPER. Max memory: 15.992 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\unsloth\\models\\llama.py:1255: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "893302f240704c4db45293d40b4e7e86"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../DeepSeek-R1-Distill-Qwen-7B does not have a padding token! Will use pad_token = <|vision_pad|>.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "3ea8ca04-2d64-40a3-9df0-89e9dbaca0fe",
   "metadata": {},
   "source": "> åœ¨INT4é‡åŒ–æƒ…å†µä¸‹ï¼Œ8Bæ¨¡å‹æ¨ç†ä»…éœ€7Gå·¦å³æ˜¾å­˜ã€‚"
  },
  {
   "cell_type": "markdown",
   "id": "531e19d8-365d-4464-badb-1bc1d6ec77dd",
   "metadata": {},
   "source": "æ­¤æ—¶modelå°±æ˜¯è¯»å–è¿›æ¥çš„DeepSeek R1 8Bè’¸é¦æ¨¡å‹ï¼š"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:32:49.305398Z",
     "start_time": "2025-02-18T08:32:49.299299Z"
    }
   },
   "cell_type": "code",
   "source": "model.to(\"cuda\")",
   "id": "cf69f27fd22e1c00",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "f687092f-94c7-450a-b251-fceb26e5a716",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-17T08:58:47.590738Z",
     "start_time": "2025-02-17T08:58:47.588714Z"
    }
   },
   "source": "# model",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "bd41d797-5505-415d-93f8-2e6adaf4f961",
   "metadata": {},
   "source": "è€Œtokenizeråˆ™æ˜¯åˆ†è¯å™¨ï¼š"
  },
  {
   "cell_type": "code",
   "id": "46bac590-0f5b-4c71-98ea-320c00d69b12",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-16T13:18:20.168443Z",
     "start_time": "2025-02-16T13:18:20.165949Z"
    }
   },
   "source": "# tokenizer",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "1a7998c2-3505-45e6-98bd-e913609b0530",
   "metadata": {},
   "source": "å°†æ¨¡å‹è°ƒæ•´ä¸ºæ¨ç†æ¨¡å¼ï¼š"
  },
  {
   "cell_type": "code",
   "id": "443cad53-991e-4904-8c03-e06259020939",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-18T06:29:58.408241Z",
     "start_time": "2025-02-18T06:29:58.402793Z"
    }
   },
   "source": "FastLanguageModel.for_inference(model)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "71a80018-7759-44b1-9ec1-88c5a62a1e7c",
   "metadata": {},
   "source": "ç„¶åå³å¯å’Œæ¨¡å‹è¿›è¡Œå¯¹è¯ï¼š"
  },
  {
   "cell_type": "code",
   "id": "126c1a9f-12a7-4144-afc3-3f0d7ab47fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:02:26.805233Z",
     "start_time": "2025-02-18T06:02:26.802395Z"
    }
   },
   "source": "question = \"è¯·ä»‹ç»ä½ è‡ªå·±ï¼ŸåŒ…æ‹¬å…´è¶£ã€ç‰¹é•¿ã€å·¥ä½œç»å†ç­‰ã€‚å¤§æ¦‚200å­—å·¦å³ã€‚\"",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "efd9cdc7-9f6e-466a-8a8d-b88ce06b69ec",
   "metadata": {},
   "source": "ç„¶åè¿™é‡Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å€ŸåŠ©åˆ†è¯å™¨ï¼Œå°†è¾“å…¥çš„é—®é¢˜è½¬åŒ–ä¸ºæ ‡è®°ç´¢å¼•ï¼š"
  },
  {
   "cell_type": "code",
   "id": "f85aa14b-f4f2-45c8-90bd-238b875b16a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:02:28.596516Z",
     "start_time": "2025-02-18T06:02:28.592494Z"
    }
   },
   "source": "inputs = tokenizer([question], return_tensors=\"pt\").to(\"cuda\")",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "ae01d5b0-542b-4d58-9209-bf5f265a2039",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T14:05:23.700185Z",
     "start_time": "2025-02-17T14:05:23.695967Z"
    }
   },
   "source": "inputs",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151646,  14880, 100157, 107828,  11319, 100630, 100565,   5373, 112526,\n",
       "           5373,  99257, 100798,  49567,   1773, 102201,     17,     15,     15,\n",
       "          18600, 101081,   1773]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "7e9c2f92-5814-47a2-9c8f-244d828344b3",
   "metadata": {},
   "source": "æœ€åå†å¸¦å…¥inputsè¿›è¡Œå¯¹è¯"
  },
  {
   "cell_type": "code",
   "id": "0e2d68a5-56c3-478d-9c6b-63095fe38660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:03:00.105048Z",
     "start_time": "2025-02-18T06:02:32.129310Z"
    }
   },
   "source": [
    "# è°ƒæ•´ç”Ÿæˆå‚æ•°\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    max_new_tokens=500,  # é™åˆ¶ç”Ÿæˆçš„é•¿åº¦\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    temperature=0.7,  # æ§åˆ¶éšæœºæ€§\n",
    "    use_cache=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "2c8ce6c0-871f-42dc-a6d6-ca694464580d",
   "metadata": {},
   "source": "æ­¤æ—¶å¾—åˆ°çš„å›å¤ä¹Ÿæ˜¯è¯ç´¢å¼•ï¼š"
  },
  {
   "cell_type": "code",
   "id": "5aedeeac-5163-4a3a-b33e-038ed93bf1fe",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-16T13:19:13.346993Z",
     "start_time": "2025-02-16T13:19:13.339913Z"
    }
   },
   "source": "# outputs",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "932f33d9-ebbd-4ed9-bce5-14802d18ec67",
   "metadata": {},
   "source": "åŒæ ·éœ€è¦åˆ†è¯å™¨å°†å…¶è½¬åŒ–ä¸ºæ–‡æœ¬ï¼š"
  },
  {
   "cell_type": "code",
   "id": "328a281f-cc79-4e79-b765-b12603f23040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:03:00.111058Z",
     "start_time": "2025-02-18T06:03:00.108046Z"
    }
   },
   "source": "response = tokenizer.batch_decode(outputs,skip_special_tokens=True)",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "080d7add-cfa5-4a5a-88bc-e1b498fd050e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:03:00.136019Z",
     "start_time": "2025-02-18T06:03:00.132508Z"
    }
   },
   "source": "response",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['è¯·ä»‹ç»ä½ è‡ªå·±ï¼ŸåŒ…æ‹¬å…´è¶£ã€ç‰¹é•¿ã€å·¥ä½œç»å†ç­‰ã€‚å¤§æ¦‚200å­—å·¦å³ã€‚çœ‹èµ·æ¥æŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒºæŒº']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "58b8005e-a3df-43b4-bf07-6d189457276f",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-17T14:05:57.230137Z",
     "start_time": "2025-02-17T14:05:57.227880Z"
    }
   },
   "source": "print(response[0])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯·ä»‹ç»ä½ è‡ªå·±ï¼ŸåŒ…æ‹¬å…´è¶£ã€ç‰¹é•¿ã€å·¥ä½œç»å†ç­‰ã€‚å¤§æ¦‚200å­—å·¦å³ã€‚ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰ï¼‰\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "0b4f7d8b-ae44-4e9d-a386-5a9645456719",
   "metadata": {},
   "source": "è‡³æ­¤æˆ‘ä»¬å°±å®Œæˆäº†unslothæ¨¡å‹æ¨ç†æµç¨‹ã€‚"
  },
  {
   "cell_type": "markdown",
   "id": "f22e138d-3029-4e00-bd2a-c84017cee398",
   "metadata": {},
   "source": "- å°è¯•ä½¿ç”¨unslothè°ƒç”¨Qwenæ¨¡å‹"
  },
  {
   "cell_type": "markdown",
   "id": "b2a82a8c-69f8-4bea-8f30-ad4fd3c12c0d",
   "metadata": {},
   "source": "&emsp;&emsp;ç±»ä¼¼çš„ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨unslothè°ƒç”¨Qwenè’¸é¦æ¨¡å‹"
  },
  {
   "cell_type": "code",
   "id": "d2b1ccf8-7fc1-4eb1-bbb2-cb75b645f4b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:20:01.003453Z",
     "start_time": "2025-02-18T06:20:00.699580Z"
    }
   },
   "source": "model_qwen, tokenizer_qwen = FastLanguageModel.from_pretrained(\n    model_name = \"../DeepSeek-R1-Distill-Qwen-7B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FastLanguageModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model_qwen, tokenizer_qwen \u001B[38;5;241m=\u001B[39m FastLanguageModel\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m      2\u001B[0m     model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../DeepSeek-R1-Distill-Qwen-7B\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      3\u001B[0m     max_seq_length \u001B[38;5;241m=\u001B[39m max_seq_length,\n\u001B[0;32m      4\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtype,\n\u001B[0;32m      5\u001B[0m     load_in_4bit \u001B[38;5;241m=\u001B[39m load_in_4bit,\n\u001B[0;32m      6\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'FastLanguageModel' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "6ead11f1-fe6d-4de7-abf1-daa30539b357",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-16T11:53:01.551285Z",
     "start_time": "2025-02-16T11:53:01.543026Z"
    }
   },
   "source": "FastLanguageModel.for_inference(model_qwen) ",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "1ee0518f-7b80-4ddf-a00f-d804da59cf64",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-16T11:53:39.123482Z",
     "start_time": "2025-02-16T11:53:05.489900Z"
    }
   },
   "source": "inputs = tokenizer_qwen([question], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model_qwen.generate(\n    input_ids=inputs.input_ids,\n    max_new_tokens=1200,\n    use_cache=True,\n)\n\nresponse = tokenizer_qwen.batch_decode(outputs)\n\nprint(response[0])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ï½œbeginâ–ofâ–sentenceï½œ>è¯·é—®å¦‚ä½•è¯æ˜æ ¹å·2æ˜¯æ— ç†æ•°ï¼Ÿâ€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "137554f3-0ccf-4965-be04-896b41cf4022",
   "metadata": {},
   "source": "> æ³¨ï¼Œä»¥ä¸‹å®éªŒå‡ä»¥DeepSeek R1 LLama3è’¸é¦æ¨¡å‹ä¸ºä¾‹è¿›è¡Œæ¼”ç¤ºå’Œè®²è§£ï¼Œè‹¥æƒ³æ›¿æ¢ä¸ºDeepSeek R1 Qwenæ¨¡å‹ï¼Œåˆ™å¯ä»¥ç›´æ¥æ›¿æ¢æ¨¡å‹åç§°å³å¯ã€‚"
  },
  {
   "cell_type": "markdown",
   "id": "ea63eb8c-ace2-4287-b924-6d8783ad2c95",
   "metadata": {},
   "source": "#### 2.å¸¦å…¥é—®ç­”æ¨¡æ¿è¿›è¡Œå›ç­”"
  },
  {
   "cell_type": "markdown",
   "id": "e10aee95-a38d-4e3b-9a81-3cf1050a1ecf",
   "metadata": {},
   "source": "- ç»“æ„åŒ–è¾“å…¥æ–¹æ³•"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:11:09.054340Z",
     "start_time": "2025-02-16T13:11:09.050654Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_style_chat = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"",
   "id": "a7e881178f7bd96c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "82b427d8-6931-406f-a2ab-38fdb38b613e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:15:51.513066Z",
     "start_time": "2025-02-16T13:15:51.500131Z"
    }
   },
   "source": "prompt_style_chat = \"\"\"è¯·å†™å‡ºä¸€ä¸ªæ°å½“çš„å›ç­”æ¥å®Œæˆå½“å‰å¯¹è¯ä»»åŠ¡ã€‚\n\n### Instruction:\nä½ æ˜¯ä¸€ååŠ©äººä¸ºä¹çš„åŠ©æ‰‹ã€‚\n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"",
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "c9fc959f-0190-4307-aa0b-43d26a81b033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:11:11.202301Z",
     "start_time": "2025-02-16T13:11:11.198777Z"
    }
   },
   "source": "question = \"ä¸€ä¸ªæ‚£æœ‰æ€¥æ€§é˜‘å°¾ç‚çš„ç—…äººå·²ç»å‘ç—…5å¤©äº†ï¼Œè…¹ç—›å°‘æœ‰å‡è½»ä½†ä»»ç„¶å‘çƒ­ï¼Œåœ¨ä½“æ£€æ—¶å‘ç°å³ä¸‹è…¹æœ‰å‹ç—›çš„åŒ…å—ï¼Œæ­¤æ—¶åº”è¯¥å¦‚ä½•å¤„ç†ï¼Ÿ\"",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "f5d63aa1-48da-4173-87f0-12a334596f5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:15:54.341737Z",
     "start_time": "2025-02-16T13:15:54.335170Z"
    }
   },
   "source": "[prompt_style_chat.format(question, \"\")]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['è¯·å†™å‡ºä¸€ä¸ªæ°å½“çš„å›ç­”æ¥å®Œæˆå½“å‰å¯¹è¯ä»»åŠ¡ã€‚\\n\\n### Instruction:\\nä½ æ˜¯ä¸€ååŠ©äººä¸ºä¹çš„åŠ©æ‰‹ã€‚\\n\\n### Question:\\nè¯·è¯æ˜æ ¹å·2æ˜¯æ— ç†æ•°ã€‚\\n\\n### Response:\\n<think>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "f9fe11da-3433-4c80-8723-861cce6ee031",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:11:15.888592Z",
     "start_time": "2025-02-16T13:11:15.883778Z"
    }
   },
   "source": "inputs = tokenizer([prompt_style_chat.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "f45b4bf8-c61f-4276-bfed-86a28b34fb93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:11:43.459997Z",
     "start_time": "2025-02-16T13:11:17.960518Z"
    }
   },
   "source": "outputs = model.generate(\n    input_ids=inputs.input_ids,\n    max_new_tokens=1200,\n    use_cache=True,\n    attention_mask=inputs.attention_mask,\n)",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "1be62f68-2c81-4056-b678-948a5bc893f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:11:43.465784Z",
     "start_time": "2025-02-16T13:11:43.462200Z"
    }
   },
   "source": "response = tokenizer.batch_decode(outputs)",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "c7459285-9821-4828-a26f-9c9c70b42420",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:11:43.606609Z",
     "start_time": "2025-02-16T13:11:43.483544Z"
    }
   },
   "source": "response",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<ï½œbeginâ–ofâ–sentenceï½œ>Below is an instruction that describes a task, paired with an input that provides further context.\\nWrite a response that appropriately completes the request.\\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\\nPlease answer the following medical question.\\n\\n### Question:\\nä¸€ä¸ªæ‚£æœ‰æ€¥æ€§é˜‘å°¾ç‚çš„ç—…äººå·²ç»å‘ç—…5å¤©äº†ï¼Œè…¹ç—›å°‘æœ‰å‡è½»ä½†ä»»ç„¶å‘çƒ­ï¼Œåœ¨ä½“æ£€æ—¶å‘ç°å³ä¸‹è…¹æœ‰å‹ç—›çš„åŒ…å—ï¼Œæ­¤æ—¶åº”è¯¥å¦‚ä½•å¤„ç†ï¼Ÿ\\n\\n### Response:\\n<think>\\n\\nè¦å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦äº†è§£ç—…äººçš„æƒ…å†µå’Œè¯Šæ–­ç¨‹åºã€‚\\n\\nè¯Šæ–­è¿‡ç¨‹åŒ…æ‹¬ï¼š\\n1. ç—…äººæ­£åœ¨æœç”¨è¯ç‰©å’Œè¿›è¡Œè¯Šæ–­ã€‚\\n2. æ£€æŸ¥èº«ä½“æŒ‡æ ‡ï¼Œå¦‚è¡€å‹ã€ pulseã€å’Œç°åœ¨çš„æƒ…å†µã€‚\\n3. æ£€æŸ¥èº«ä½“æ˜¯å¦æœ‰å¼‚å¸¸ï¼Œå¦‚æ„ŸæŸ“ã€ç‚ç—‡ã€æˆ–è¯ç‰©å½±å“ã€‚\\n4. è€ƒè™‘è¯ç‰©å’Œè¯Šæ–­ç¨‹åºçš„ç»¼åˆå½±å“ã€‚\\n\\nè¯Šæ–­æ­¥éª¤ï¼š\\n- è¯„ä¼°è¯ç‰©å½±å“å’Œè¯Šæ–­ç¨‹åºã€‚\\n- è€ƒè™‘ç»¼åˆå½±å“å’Œè¯Šæ–­ç»“æœã€‚\\n- æ£€æŸ¥èº«ä½“æŒ‡æ ‡ï¼Œå¦‚è¡€å‹ã€ pulseã€å’Œç°åœ¨çš„æƒ…å†µã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºåŒ…æ‹¬ï¼š\\n1. è¯„ä¼°è¯ç‰©å½±å“å’Œè¯Šæ–­ç¨‹åºçš„ç»¼åˆå½±å“ã€‚\\n2. è€ƒè™‘ç»¼åˆå½±å“å’Œè¯Šæ–­ç»“æœã€‚\\n3. æ£€æŸ¥èº«ä½“æŒ‡æ ‡ï¼Œå¦‚è¡€å‹ã€ pulseã€å’Œç°åœ¨çš„æƒ…å†µã€‚\\n\\næœ€ç»ˆï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºåŒ…æ‹¬è¯„ä¼°è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\næœ€ç»ˆï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºåŒ…æ‹¬è¯„ä¼°è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\næœ€ç»ˆï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºåŒ…æ‹¬è¯„ä¼°è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\næœ€ç»ˆï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\\n\\nå› æ­¤ï¼Œè¯Šæ–­ç¨‹åº']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:12:24.933869Z",
     "start_time": "2025-02-16T13:12:24.928674Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(response[0].split(\"### Response:\")[1]))",
   "id": "ecf1520aade0a754",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2084\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "16ee5204-8c35-4e63-9e19-2a33894a4b9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:11:43.627820Z",
     "start_time": "2025-02-16T13:11:43.624938Z"
    }
   },
   "source": "print(response[0].split(\"### Response:\")[1])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "\n",
      "è¦å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦äº†è§£ç—…äººçš„æƒ…å†µå’Œè¯Šæ–­ç¨‹åºã€‚\n",
      "\n",
      "è¯Šæ–­è¿‡ç¨‹åŒ…æ‹¬ï¼š\n",
      "1. ç—…äººæ­£åœ¨æœç”¨è¯ç‰©å’Œè¿›è¡Œè¯Šæ–­ã€‚\n",
      "2. æ£€æŸ¥èº«ä½“æŒ‡æ ‡ï¼Œå¦‚è¡€å‹ã€ pulseã€å’Œç°åœ¨çš„æƒ…å†µã€‚\n",
      "3. æ£€æŸ¥èº«ä½“æ˜¯å¦æœ‰å¼‚å¸¸ï¼Œå¦‚æ„ŸæŸ“ã€ç‚ç—‡ã€æˆ–è¯ç‰©å½±å“ã€‚\n",
      "4. è€ƒè™‘è¯ç‰©å’Œè¯Šæ–­ç¨‹åºçš„ç»¼åˆå½±å“ã€‚\n",
      "\n",
      "è¯Šæ–­æ­¥éª¤ï¼š\n",
      "- è¯„ä¼°è¯ç‰©å½±å“å’Œè¯Šæ–­ç¨‹åºã€‚\n",
      "- è€ƒè™‘ç»¼åˆå½±å“å’Œè¯Šæ–­ç»“æœã€‚\n",
      "- æ£€æŸ¥èº«ä½“æŒ‡æ ‡ï¼Œå¦‚è¡€å‹ã€ pulseã€å’Œç°åœ¨çš„æƒ…å†µã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºåŒ…æ‹¬ï¼š\n",
      "1. è¯„ä¼°è¯ç‰©å½±å“å’Œè¯Šæ–­ç¨‹åºçš„ç»¼åˆå½±å“ã€‚\n",
      "2. è€ƒè™‘ç»¼åˆå½±å“å’Œè¯Šæ–­ç»“æœã€‚\n",
      "3. æ£€æŸ¥èº«ä½“æŒ‡æ ‡ï¼Œå¦‚è¡€å‹ã€ pulseã€å’Œç°åœ¨çš„æƒ…å†µã€‚\n",
      "\n",
      "æœ€ç»ˆï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºåŒ…æ‹¬è¯„ä¼°è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "æœ€ç»ˆï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºåŒ…æ‹¬è¯„ä¼°è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "æœ€ç»ˆï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºåŒ…æ‹¬è¯„ä¼°è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "æœ€ç»ˆï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åºæ˜¯è¯ç‰©å½±å“å’Œè¯Šæ–­æ­¥éª¤çš„ç»¼åˆå½±å“ï¼Œä»¥åŠèº«ä½“æŒ‡æ ‡çš„æ£€æŸ¥ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¯Šæ–­ç¨‹åº\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "8fc39741-01df-4adb-88fb-885bc4340e2e",
   "metadata": {},
   "source": "- å¤æ‚é—®é¢˜æµ‹è¯•"
  },
  {
   "cell_type": "code",
   "id": "1c89626c-b70a-42a5-8a91-c21c8b28cca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:16:00.821035Z",
     "start_time": "2025-02-16T13:16:00.817987Z"
    }
   },
   "source": "question = \"è¯·è¯æ˜æ ¹å·2æ˜¯æ— ç†æ•°ã€‚\"",
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "62ad1369-ccd4-4885-872a-686277148c75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:16:01.115555Z",
     "start_time": "2025-02-16T13:16:01.110868Z"
    }
   },
   "source": "inputs = tokenizer([prompt_style_chat.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")",
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "53a02c58-9c03-4e22-bb38-d5f8eb13d3af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:16:27.262904Z",
     "start_time": "2025-02-16T13:16:01.490956Z"
    }
   },
   "source": "outputs = model.generate(\n    input_ids=inputs.input_ids,\n    max_new_tokens=1200,\n    use_cache=True,\n    attention_mask=inputs.attention_mask,\n)",
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "723a82fc-2e5e-48c4-9460-1bf9d1bd2df3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:16:27.274312Z",
     "start_time": "2025-02-16T13:16:27.262904Z"
    }
   },
   "source": "response = tokenizer.batch_decode(outputs)",
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "6c862fa3-ffe2-4736-829d-8f201716dddd",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-16T13:16:44.748073Z",
     "start_time": "2025-02-16T13:16:44.745098Z"
    }
   },
   "source": "print(response[0].split(\"### Response:\")[1])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<ï½œbeginâ–ofâ–sentenceï½œ>è¯·å†™å‡ºä¸€ä¸ªæ°å½“çš„å›ç­”æ¥å®Œæˆå½“å‰å¯¹è¯ä»»åŠ¡ã€‚\\n\\n### Instruction:\\nä½ æ˜¯ä¸€ååŠ©äººä¸ºä¹çš„åŠ©æ‰‹ã€‚\\n\\n### Question:\\nè¯·è¯æ˜æ ¹å·2æ˜¯æ— ç†æ•°ã€‚\\n\\n### Response:\\n<think>\\n\\n</think>\\n\\næ‚¨çš„è¯·æ±‚å·²ç»å¾—åˆ°æ»¡è¶³ã€‚\\n\\n### Command\\n\\nCommand\\n\\n### Question\\n\\nQuestion\\n\\n### Question\\n\\nQuestion\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n### Question\\n\\n']\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "4258079b-67cd-43ef-8e5e-c8d20c6cc62f",
   "metadata": {},
   "source": "#### 3.åŸå§‹æ¨¡å‹çš„åŒ»ç–—é—®é¢˜é—®ç­”"
  },
  {
   "cell_type": "markdown",
   "id": "7d7dda0e-4a98-4ded-9be8-cc60ffb9f356",
   "metadata": {},
   "source": "- é‡æ–°è®¾ç½®é—®ç­”æ¨¡æ¿"
  },
  {
   "cell_type": "code",
   "id": "4dadf6c0-53c1-4d07-b0ec-57b723198f4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:09:39.704952Z",
     "start_time": "2025-02-18T08:09:39.701336Z"
    }
   },
   "source": "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "153b915e-914e-41e4-b6be-d81ab77a7c8f",
   "metadata": {},
   "source": "ç¿»è¯‘å¦‚ä¸‹ï¼š\n\n```python\nprompt_style = \"\"\"ä»¥ä¸‹æ˜¯ä¸€ä¸ªä»»åŠ¡è¯´æ˜ï¼Œé…æœ‰æä¾›æ›´å¤šèƒŒæ™¯ä¿¡æ¯çš„è¾“å…¥ã€‚\nè¯·å†™å‡ºä¸€ä¸ªæ°å½“çš„å›ç­”æ¥å®Œæˆè¯¥ä»»åŠ¡ã€‚\nåœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶æŒ‰æ­¥éª¤è¿›è¡Œæ¨ç†ï¼Œç¡®ä¿å›ç­”é€»è¾‘æ¸…æ™°ä¸”å‡†ç¡®ã€‚\n\n### Instruction:\næ‚¨æ˜¯ä¸€ä½å…·æœ‰é«˜çº§ä¸´åºŠæ¨ç†ã€è¯Šæ–­å’Œæ²»ç–—è§„åˆ’çŸ¥è¯†çš„åŒ»å­¦ä¸“å®¶ã€‚\nè¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\n```"
  },
  {
   "cell_type": "markdown",
   "id": "1ea90929-b517-414c-8399-0a6f783b7c26",
   "metadata": {},
   "source": "&emsp;&emsp;æ¥ä¸‹æ¥æˆ‘ä»¬æŠ½å–éƒ¨åˆ†medical-o1-reasoning-SFTæ•°æ®é›†ä¸­é—®é¢˜è¿›è¡Œæé—®ï¼Œå¹¶æŸ¥çœ‹åˆå§‹çŠ¶æ€ä¸‹æ¨¡å‹å›ç­”ç»“æœã€‚"
  },
  {
   "cell_type": "code",
   "id": "fee56999-513f-4a2b-9be5-677821c994f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:09:41.542040Z",
     "start_time": "2025-02-18T08:09:41.538757Z"
    }
   },
   "source": "question_1 = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"",
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "c4e98f9e-6c5b-42f3-8e78-07951cdc069c",
   "metadata": {},
   "source": "ç¿»è¯‘ï¼šä¸€ä½61å²çš„å¥³æ€§ï¼Œæœ‰é•¿æœŸåœ¨å’³å—½æˆ–æ‰“å–·åšç­‰æ´»åŠ¨ä¸­å‘ç”Ÿä¸è‡ªä¸»å°¿æ¶²æµå¤±çš„ç—…å²ï¼Œä½†å¤œé—´æ²¡æœ‰æ¼å°¿ã€‚å¥¹æ¥å—äº†å¦‡ç§‘æ£€æŸ¥å’ŒQ-tipæµ‹è¯•ã€‚æ ¹æ®è¿™äº›æ£€æŸ¥ç»“æœï¼Œè†€èƒ±æµ‹é‡ï¼ˆcystometryï¼‰æœ€å¯èƒ½ä¼šæ˜¾ç¤ºå¥¹çš„æ®‹ä½™å°¿é‡å’Œé€¼å°¿è‚Œæ”¶ç¼©æƒ…å†µå¦‚ä½•ï¼Ÿ\n\n\n\n\n\n\n\n"
  },
  {
   "cell_type": "code",
   "id": "96bffeb4-8252-4e4e-8c3e-9a3375ab940f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:29:42.016619Z",
     "start_time": "2025-02-18T06:29:42.013537Z"
    }
   },
   "source": "question_2 = \"Given a patient who experiences sudden-onset chest pain radiating to the neck and left arm, with a past medical history of hypercholesterolemia and coronary artery disease, elevated troponin I levels, and tachycardia, what is the most likely coronary artery involved based on this presentation?\"",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "6623b554-cae5-4a9f-b033-9c356ae031ad",
   "metadata": {},
   "source": "ç¿»è¯‘ï¼šé¢å¯¹ä¸€ä½çªå‘èƒ¸ç—›å¹¶æ”¾å°„è‡³é¢ˆéƒ¨å’Œå·¦è‡‚çš„æ‚£è€…ï¼Œå…¶æ—¢å¾€ç—…å²åŒ…æ‹¬é«˜èƒ†å›ºé†‡è¡€ç—‡å’Œå† çŠ¶åŠ¨è„‰ç–¾ç—…ï¼ŒåŒæ—¶ä¼´æœ‰å‡é«˜çš„è‚Œé’™è›‹ç™½Iæ°´å¹³å’Œå¿ƒåŠ¨è¿‡é€Ÿï¼Œæ ¹æ®è¿™äº›ä¸´åºŠè¡¨ç°ï¼Œæœ€å¯èƒ½å—ç´¯çš„å† çŠ¶åŠ¨è„‰æ˜¯å“ªä¸€æ¡ï¼Ÿ"
  },
  {
   "cell_type": "markdown",
   "id": "f7e929f7-614c-4f86-9cd0-b6417c1adbd1",
   "metadata": {},
   "source": "- é—®ç­”æµ‹è¯•"
  },
  {
   "cell_type": "code",
   "id": "3c2290e9-1e9e-4fef-bde8-cc84591437bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:31:43.026351Z",
     "start_time": "2025-02-18T06:30:05.499122Z"
    }
   },
   "source": "inputs1 = tokenizer([prompt_style.format(question_1, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n\noutputs1 = model.generate(\n    input_ids=inputs1.input_ids,\n    max_new_tokens=1200,\n    use_cache=True,\n    attention_mask=inputs1.attention_mask,\n)\n\nresponse1 = tokenizer.batch_decode(outputs1)",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m inputs1 \u001B[38;5;241m=\u001B[39m tokenizer([prompt_style\u001B[38;5;241m.\u001B[39mformat(question_1, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)], return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m outputs1 \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[0;32m      5\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minputs1\u001B[38;5;241m.\u001B[39minput_ids,\n\u001B[0;32m      6\u001B[0m     max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1200\u001B[39m,\n\u001B[0;32m      7\u001B[0m     use_cache\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m      8\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39minputs1\u001B[38;5;241m.\u001B[39mattention_mask,\n\u001B[0;32m      9\u001B[0m )\n\u001B[0;32m     11\u001B[0m response1 \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mbatch_decode(outputs1)\n",
      "File \u001B[1;32mE:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\unsloth\\models\\llama.py:1574\u001B[0m, in \u001B[0;36m_wrap_fast_inference.<locals>._fast_generate\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1567\u001B[0m \u001B[38;5;66;03m# Set pad token\u001B[39;00m\n\u001B[0;32m   1568\u001B[0m \u001B[38;5;66;03m# old_pad_token_id = getattr(model.config, \"pad_token_id\", None)\u001B[39;00m\n\u001B[0;32m   1569\u001B[0m \u001B[38;5;66;03m# old_eos_token_id = getattr(model.config, \"eos_token_id\", None)\u001B[39;00m\n\u001B[0;32m   1570\u001B[0m \u001B[38;5;66;03m# model.config.pad_token_id = old_eos_token_id\u001B[39;00m\n\u001B[0;32m   1571\u001B[0m \n\u001B[0;32m   1572\u001B[0m \u001B[38;5;66;03m# Autocasted\u001B[39;00m\n\u001B[0;32m   1573\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautocast(device_type \u001B[38;5;241m=\u001B[39m device_type, dtype \u001B[38;5;241m=\u001B[39m dtype):\n\u001B[1;32m-> 1574\u001B[0m     output \u001B[38;5;241m=\u001B[39m generate(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1575\u001B[0m \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1577\u001B[0m \u001B[38;5;66;03m# Revert\u001B[39;00m\n\u001B[0;32m   1578\u001B[0m \u001B[38;5;66;03m# model.config.pad_token_id = old_pad_token_id\u001B[39;00m\n\u001B[0;32m   1579\u001B[0m \n\u001B[0;32m   1580\u001B[0m \u001B[38;5;66;03m# Unset a flag for generation!\u001B[39;00m\n",
      "File \u001B[1;32mE:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\transformers\\generation\\utils.py:2255\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   2247\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   2248\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   2249\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[0;32m   2250\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   2251\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2252\u001B[0m     )\n\u001B[0;32m   2254\u001B[0m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[1;32m-> 2255\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sample(\n\u001B[0;32m   2256\u001B[0m         input_ids,\n\u001B[0;32m   2257\u001B[0m         logits_processor\u001B[38;5;241m=\u001B[39mprepared_logits_processor,\n\u001B[0;32m   2258\u001B[0m         stopping_criteria\u001B[38;5;241m=\u001B[39mprepared_stopping_criteria,\n\u001B[0;32m   2259\u001B[0m         generation_config\u001B[38;5;241m=\u001B[39mgeneration_config,\n\u001B[0;32m   2260\u001B[0m         synced_gpus\u001B[38;5;241m=\u001B[39msynced_gpus,\n\u001B[0;32m   2261\u001B[0m         streamer\u001B[38;5;241m=\u001B[39mstreamer,\n\u001B[0;32m   2262\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2263\u001B[0m     )\n\u001B[0;32m   2265\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[0;32m   2266\u001B[0m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[0;32m   2267\u001B[0m     beam_scorer \u001B[38;5;241m=\u001B[39m BeamSearchScorer(\n\u001B[0;32m   2268\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   2269\u001B[0m         num_beams\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2274\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[0;32m   2275\u001B[0m     )\n",
      "File \u001B[1;32mE:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\transformers\\generation\\utils.py:3257\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[0;32m   3255\u001B[0m     is_prefill \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   3256\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m model_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs, return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   3259\u001B[0m \u001B[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001B[39;00m\n\u001B[0;32m   3260\u001B[0m model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_model_kwargs_for_generation(\n\u001B[0;32m   3261\u001B[0m     outputs,\n\u001B[0;32m   3262\u001B[0m     model_kwargs,\n\u001B[0;32m   3263\u001B[0m     is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   3264\u001B[0m )\n",
      "File \u001B[1;32mE:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\unsloth\\models\\llama.py:1037\u001B[0m, in \u001B[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001B[1;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1019\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_CausalLM_fast_forward\u001B[39m(\n\u001B[0;32m   1020\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1021\u001B[0m     input_ids: torch\u001B[38;5;241m.\u001B[39mLongTensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1033\u001B[0m     \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1034\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001B[0;32m   1036\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m past_key_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1037\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m fast_forward_inference(\n\u001B[0;32m   1038\u001B[0m             \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1039\u001B[0m             input_ids,\n\u001B[0;32m   1040\u001B[0m             past_key_values,\n\u001B[0;32m   1041\u001B[0m             position_ids \u001B[38;5;241m=\u001B[39m position_ids,\n\u001B[0;32m   1042\u001B[0m             attention_mask \u001B[38;5;241m=\u001B[39m attention_mask,\n\u001B[0;32m   1043\u001B[0m         )\n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1045\u001B[0m         causal_mask \u001B[38;5;241m=\u001B[39m xformers\u001B[38;5;241m.\u001B[39mattn_bias\u001B[38;5;241m.\u001B[39mLowerTriangularMask() \u001B[38;5;28;01mif\u001B[39;00m HAS_XFORMERS \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mE:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\unsloth\\models\\llama.py:991\u001B[0m, in \u001B[0;36mLlamaModel_fast_forward_inference\u001B[1;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001B[0m\n\u001B[0;32m    983\u001B[0m residual\u001B[38;5;241m.\u001B[39mcopy_(X) \u001B[38;5;66;03m# residual = X\u001B[39;00m\n\u001B[0;32m    984\u001B[0m X \u001B[38;5;241m=\u001B[39m fast_rms_layernorm_inference(\n\u001B[0;32m    985\u001B[0m     decoder_layer\u001B[38;5;241m.\u001B[39mpost_attention_layernorm,\n\u001B[0;32m    986\u001B[0m     X,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    989\u001B[0m     variance \u001B[38;5;241m=\u001B[39m variance,\n\u001B[0;32m    990\u001B[0m )\n\u001B[1;32m--> 991\u001B[0m X \u001B[38;5;241m=\u001B[39m fast_swiglu_inference(\n\u001B[0;32m    992\u001B[0m     decoder_layer\u001B[38;5;241m.\u001B[39mmlp,\n\u001B[0;32m    993\u001B[0m     X,\n\u001B[0;32m    994\u001B[0m     temp_gate \u001B[38;5;241m=\u001B[39m temp_gate,\n\u001B[0;32m    995\u001B[0m     temp_up \u001B[38;5;241m=\u001B[39m temp_up,\n\u001B[0;32m    996\u001B[0m )\n\u001B[0;32m    997\u001B[0m X \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m residual\n\u001B[0;32m    999\u001B[0m next_decoder_cache\u001B[38;5;241m.\u001B[39mappend(present_key_value)\n",
      "File \u001B[1;32mE:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\unsloth\\models\\llama.py:289\u001B[0m, in \u001B[0;36mfast_swiglu_inference\u001B[1;34m(self, X, temp_gate, temp_up)\u001B[0m\n\u001B[0;32m    285\u001B[0m bsz, _, hd \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# mlp_size = self.config.intermediate_size\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda:0\")\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m gate \u001B[38;5;241m=\u001B[39m fast_linear_forward(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgate_proj, X, out \u001B[38;5;241m=\u001B[39m temp_gate)\n\u001B[0;32m    290\u001B[0m up   \u001B[38;5;241m=\u001B[39m fast_linear_forward(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m  up_proj, X, out \u001B[38;5;241m=\u001B[39m temp_up)\n\u001B[0;32m    291\u001B[0m gate \u001B[38;5;241m=\u001B[39m torch_nn_functional_silu(gate, inplace \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32mE:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\unsloth\\kernels\\utils.py:410\u001B[0m, in \u001B[0;36mfast_linear_forward\u001B[1;34m(proj, X, temp_lora, out)\u001B[0m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m q_len \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m: \u001B[38;5;28;01mreturn\u001B[39;00m matmul_lora(X, W, W_quant, lora_A, lora_B, lora_S)\n\u001B[0;32m    409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m W_quant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 410\u001B[0m     out \u001B[38;5;241m=\u001B[39m torch_matmul(X, W\u001B[38;5;241m.\u001B[39mt(), out \u001B[38;5;241m=\u001B[39m out)\n\u001B[0;32m    411\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m bsz \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m q_len \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    412\u001B[0m     out \u001B[38;5;241m=\u001B[39m fast_gemv(X, W, W_quant, out \u001B[38;5;241m=\u001B[39m out)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:31:43.055321Z",
     "start_time": "2025-02-16T13:56:06.560120Z"
    }
   },
   "cell_type": "code",
   "source": "response1",
   "id": "97ec0bf14a592309",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<ï½œbeginâ–ofâ–sentenceï½œ>Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Response:\\n<think>\\n\\n### Answer:\\n\\nThe 6-year-old woman is the patient. The patient's findings are examined. Based on the test results, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are considered. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results is used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results is used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results is used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results is used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\\n\\nThe patient's test results are used to determine the diagnosis. Based on the diagnosis\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "02e6c870-23e6-4f6f-a990-720dfc36887b",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-18T06:31:43.057321400Z",
     "start_time": "2025-02-16T13:56:12.747252Z"
    }
   },
   "source": "print(response1[0].split(\"### Response:\")[1])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "\n",
      "### Answer:\n",
      "\n",
      "The 6-year-old woman is the patient. The patient's findings are examined. Based on the test results, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are considered. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results is used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results is used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results is used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results is used to determine the diagnosis. Based on the diagnosis, what would the cyst most reveal about residual volume and andrus counter?\n",
      "\n",
      "The patient's test results are used to determine the diagnosis. Based on the diagnosis\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "b9addf3d-8181-4fbb-808f-27f1dd65f274",
   "metadata": {},
   "source": "ç¿»è¯‘å¦‚ä¸‹ï¼š\n\n\\<think>  \nå¥½çš„ï¼Œæˆ‘æ­£åœ¨å°è¯•åˆ†æè¿™ä¸ªåŒ»å­¦é—®é¢˜ã€‚æˆ‘ä»¬ä¸€æ­¥æ­¥æ¥åˆ†è§£ã€‚æ‚£è€…æ˜¯ä¸€ä½61å²çš„å¥³æ€§ï¼Œæœ‰åœ¨å’³å—½æˆ–æ‰“å–·åšç­‰æ´»åŠ¨ä¸­å‘ç”Ÿä¸è‡ªä¸»å°¿æ¶²æµå¤±çš„ç—…å²ï¼Œä½†å¥¹å¤œé—´æ²¡æœ‰æ¼å°¿ã€‚å¥¹æ­£åœ¨æ¥å—å¦‡ç§‘æ£€æŸ¥å’ŒQ-tipæµ‹è¯•ã€‚é—®é¢˜æ˜¯å…³äºè†€èƒ±æµ‹é‡ï¼ˆcystometryï¼‰ä¼šæ˜¾ç¤ºå¥¹çš„æ®‹ä½™å°¿é‡å’Œé€¼å°¿è‚Œæ”¶ç¼©æƒ…å†µã€‚\n\né¦–å…ˆï¼Œæˆ‘çŸ¥é“åœ¨åƒæ‰“å–·åšæˆ–å’³å—½ç­‰æ´»åŠ¨ä¸­å‘ç”Ÿä¸è‡ªä¸»å°¿æ¶²æµå¤±é€šå¸¸ä¸å‹åŠ›æ€§å°¿å¤±ç¦æœ‰å…³ã€‚å‹åŠ›æ€§å°¿å¤±ç¦é€šå¸¸å‘ç”Ÿåœ¨å°¿é“è‚Œè‚‰ä¸è¶³ä»¥åœ¨å‹åŠ›å¢å¤§çš„æƒ…å†µä¸‹ï¼ˆæ¯”å¦‚å’³å—½æ—¶ï¼‰é˜²æ­¢è†€èƒ±æ¼å°¿æ—¶ã€‚\n\næ¥ä¸‹æ¥æ˜¯Q-tipæµ‹è¯•ã€‚æ ¹æ®æˆ‘è®°å¾—çš„ï¼ŒQ-tipæ˜¯ä¸€ç§ç”¨äºæµ‹é‡å°¿é“å‹åŠ›æ›²çº¿çš„å°¿é“å¯¼ç®¡ã€‚å®ƒé€šå¸¸ç”¨äºè¯„ä¼°å°¿é“åŠŸèƒ½ã€‚Q-tipæµ‹è¯•é˜³æ€§ç»“æœï¼Œå³åœ¨Valsalvaæ“ä½œè¿‡ç¨‹ä¸­å°¿é“å‹åŠ›ä½äºè†€èƒ±å†…å‹ï¼Œä¸å†…æºæ€§æ‹¬çº¦è‚Œç¼ºé™·ç›¸å…³ï¼Œè¿™æ˜¯ä¸€ç§å‹åŠ›æ€§å°¿å¤±ç¦ç±»å‹ã€‚\n\nç”±äºæ‚£è€…æœ‰åœ¨æ´»åŠ¨ä¸­å‡ºç°ä¸è‡ªä¸»æ¼å°¿çš„ç—…å²ï¼Œä½†å¤œé—´æ²¡æœ‰æ¼å°¿ï¼Œæ›´å¯èƒ½æ˜¯å‹åŠ›æ€§å°¿å¤±ç¦ï¼Œè€Œä¸æ˜¯åƒæ€¥è¿«æ€§å°¿å¤±ç¦é‚£æ ·çš„æƒ…å†µï¼Œæ€¥è¿«æ€§å°¿å¤±ç¦é€šå¸¸ä¼´æœ‰å¤œé—´æ¼å°¿ã€‚å› æ­¤ï¼Œå¦‚æœQ-tipæµ‹è¯•é˜³æ€§ï¼Œæç¤ºå†…æºæ€§æ‹¬çº¦è‚Œç¼ºé™·ã€‚\n\nç°åœ¨ï¼Œè°ˆåˆ°è†€èƒ±æµ‹é‡ã€‚è†€èƒ±æµ‹é‡æ˜¯ä¸€ç§æµ‹è¯•ï¼Œæ—¨åœ¨æµ‹é‡è†€èƒ±åœ¨å……ç›ˆè¿‡ç¨‹ä¸­çš„ååº”ä»¥åŠé€¼å°¿è‚Œçš„æ”¶ç¼©æƒ…å†µã€‚å®ƒå¯ä»¥æ˜¾ç¤ºæ˜¯å¦å­˜åœ¨è†€èƒ±è¿‡åº¦æ´»åŠ¨ç—‡ï¼ˆOABï¼‰ï¼Œå³å¼•èµ·æ€¥è¿«æ„Ÿå’Œé¢‘å°¿çš„æƒ…å†µï¼Œæˆ–æ˜¯å¦å­˜åœ¨é€¼å°¿è‚Œä½æ´»åŠ¨æ€§ï¼Œå¯¼è‡´å°¿æ½´ç•™ã€‚\n\nåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚£è€…çš„ä¸»è¦é—®é¢˜æ˜¯å‹åŠ›æ€§å°¿å¤±ç¦ï¼Œè¿™æ›´ä¸æ— æ³•åœ¨å‹åŠ›å¢å¤§æ—¶ä¿æŒå°¿æ¶²æœ‰å…³ã€‚è†€èƒ±æµ‹é‡ä¼šæŸ¥çœ‹é€¼å°¿è‚Œçš„æ”¶ç¼©æƒ…å†µã€‚å¦‚æœé€¼å°¿è‚Œä½æ´»åŠ¨æ€§ï¼Œå®ƒå°†ä¸èƒ½å¼ºæœ‰åŠ›åœ°æ”¶ç¼©ä»¥æ’ç©ºè†€èƒ±ï¼Œå¯¼è‡´æ®‹ä½™å°¿é‡ã€‚ä½†å¦‚æœé€¼å°¿è‚Œè¿‡åº¦æ´»è·ƒï¼Œå¯èƒ½ä¼šæ”¶ç¼©è¿‡åº¦ï¼Œå¯¼è‡´æ€¥è¿«æ„Ÿã€‚\n\né‰´äºæ‚£è€…æœ‰å‹åŠ›æ€§å°¿å¤±ç¦çš„ç—…å²å’ŒQ-tipæµ‹è¯•é˜³æ€§ï¼Œæç¤ºå†…æºæ€§æ‹¬çº¦è‚Œç¼ºé™·ï¼Œæˆ‘è®¤ä¸ºè†€èƒ±æµ‹é‡ä¼šæ˜¾ç¤ºé€¼å°¿è‚Œçš„æ”¶ç¼©æ˜¯æ­£å¸¸çš„ã€‚é—®é¢˜ä¸åœ¨äºé€¼å°¿è‚Œæ”¶ç¼©çš„èƒ½åŠ›ï¼Œè€Œæ˜¯æ— æ³•å¯†å°å°¿é“ä»¥ä¿æŒå‹åŠ›ã€‚å› æ­¤ï¼Œæ®‹ä½™å°¿é‡å¯èƒ½æ˜¯æ­£å¸¸çš„ï¼Œé™¤éæœ‰æ˜æ˜¾çš„å°¿æ½´ç•™ï¼Œä½†å…³é”®å‘ç°æ˜¯é€¼å°¿è‚Œçš„æ”¶ç¼©æ˜¯æ­£å¸¸çš„ï¼Œè€Œä¸æ˜¯è¿‡åº¦æ´»è·ƒã€‚\n\nç­‰ç­‰ï¼Œä½†ä¼šä¸ä¼šæœ‰æ®‹ä½™å°¿é‡ï¼Ÿå¦‚æœæ‚£è€…æ’å°¿åè†€èƒ±ä¸­æ®‹ç•™ä¸€äº›å°¿æ¶²ï¼Œé‚£å°±æ˜¯æ®‹ä½™å°¿é‡ã€‚ä½†å¦‚æœæ²¡æœ‰å°¿æ½´ç•™çš„ç—‡çŠ¶ï¼Œæ¯”å¦‚è†€èƒ±é¥±èƒ€æˆ–æ’å°¿å›°éš¾ï¼Œé‚£ä¹ˆè¿™ç§æƒ…å†µçš„å¯èƒ½æ€§è¾ƒå°ã€‚ä¸»è¦é—®é¢˜æ˜¯åœ¨æ´»åŠ¨ä¸­å‘ç”Ÿçš„å°¿å¤±ç¦ï¼Œå› æ­¤é€¼å°¿è‚Œæ”¶ç¼©æ˜¯æ­£å¸¸çš„ï¼Œæ®‹ä½™å°¿é‡åœ¨æ­£å¸¸èŒƒå›´å†…ï¼Œé™¤éæœ‰å…¶ä»–æƒ…å†µã€‚\n\næ‰€ä»¥ï¼Œç»¼åˆæ¥çœ‹ï¼Œè†€èƒ±æµ‹é‡å¯èƒ½ä¼šæ˜¾ç¤ºé€¼å°¿è‚Œçš„æ”¶ç¼©æ­£å¸¸ï¼Œæ®‹ä½™å°¿é‡æ­£å¸¸ã€‚é—®é¢˜æ›´å¤šæ˜¯åœ¨æ‹¬çº¦è‚Œæ–¹é¢ï¼Œè€Œä¸æ˜¯é€¼å°¿è‚Œã€‚  \n\\</think>\n\næ ¹æ®å¯¹æ‚£è€…ç—…å²å’ŒQ-tipæµ‹è¯•ç»“æœçš„åˆ†æï¼Œè†€èƒ±æµ‹é‡æœ€å¯èƒ½æ˜¾ç¤ºé€¼å°¿è‚Œçš„æ”¶ç¼©æ­£å¸¸ï¼Œæ®‹ä½™å°¿é‡æ­£å¸¸ã€‚ä¸»è¦é—®é¢˜ä¼¼ä¹æ˜¯ç”±äºå†…æºæ€§æ‹¬çº¦è‚Œç¼ºé™·å¼•èµ·çš„å‹åŠ›æ€§å°¿å¤±ç¦ï¼Œå¦‚Q-tipæµ‹è¯•é˜³æ€§æ‰€ç¤ºã€‚è¿™ç§æƒ…å†µé€šå¸¸å½±å“å°¿é“æ‹¬çº¦è‚Œåœ¨å‹åŠ›å¢å¤§æ—¶é˜²æ­¢æ¼å°¿çš„èƒ½åŠ›ï¼Œè€Œä¸æ˜¯é€¼å°¿è‚Œçš„æ”¶ç¼©èƒ½åŠ›ã€‚å› æ­¤ï¼Œé€¼å°¿è‚Œçš„æ”¶ç¼©å¹¶æœªè¿‡åº¦æ´»è·ƒï¼Œæ®‹ä½™å°¿é‡åœ¨æ­£å¸¸èŒƒå›´å†…ã€‚"
  },
  {
   "cell_type": "markdown",
   "id": "41bf7ec2-fcd2-4b19-91c0-0eae77f42ee4",
   "metadata": {},
   "source": "æ ‡å‡†ç­”æ¡ˆï¼š\n\nåœ¨è¿™ç§å‹åŠ›æ€§å°¿å¤±ç¦çš„æƒ…å†µä¸‹ï¼Œè†€èƒ±æµ‹å‹æ£€æŸ¥ï¼ˆcystometryï¼‰æœ€å¯èƒ½æ˜¾ç¤º**æ­£å¸¸çš„æ’å°¿åæ®‹ä½™å°¿é‡**ï¼Œå› ä¸ºå‹åŠ›æ€§å°¿å¤±ç¦é€šå¸¸ä¸ä¼šå½±å“è†€èƒ±æ’ç©ºåŠŸèƒ½ã€‚æ­¤å¤–ï¼Œç”±äºå‹åŠ›æ€§å°¿å¤±ç¦ä¸»è¦ä¸**èº«ä½“ç”¨åŠ›**æœ‰å…³ï¼Œè€Œä¸æ˜¯è†€èƒ±è¿‡åº¦æ´»åŠ¨ç—‡ï¼ˆOABï¼‰ï¼Œå› æ­¤åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­**ä¸å¤ªå¯èƒ½è§‚å¯Ÿåˆ°é€¼å°¿è‚Œçš„éè‡ªä¸»æ”¶ç¼©**ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a2c20cbb-56a9-471a-94a5-b1f605bc3973",
   "metadata": {},
   "outputs": [],
   "source": "inputs2 = tokenizer([prompt_style.format(question_2, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n\noutputs2 = model.generate(\n    input_ids=inputs2.input_ids,\n    max_new_tokens=1200,\n    use_cache=True,\n)\n\nresponse2 = tokenizer.batch_decode(outputs2)"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "786cca4c-988b-4369-b9e5-56688f1ff643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, so I have a patient presenting with sudden-onset chest pain that's radiating to the neck and left arm. That makes me think of a heart attack because those symptoms are classicâ€”angina or myocardial infarction. The left arm pain, neck pain, and sometimes jaw or back pain can all be related to coronary artery issues. \n",
      "\n",
      "Looking at the past medical history, the patient has hypercholesterolemia, which is high cholesterol, and coronary artery disease. Those are both risk factors for atherosclerosis, which can lead to blockages in the coronary arteries. The elevated troponin I levels are a big clue because troponin is a cardiac enzyme released when the heart muscle is damaged, which is a sign of a heart attack. Also, the patient is experiencing tachycardia, which means their heart is beating faster than usual. In a heart attack, the heart might beat faster as it tries to pump blood to compensate for the blocked artery.\n",
      "\n",
      "Now, considering the coronary arteries, the left main coronary artery supplies blood to the entire left side of the heart, including the left ventricle, which is a large muscle that's crucial for pumping blood. If there's a blockage here, it can lead to a more severe heart attack because the left ventricle is so vital. The right coronary artery supplies the right ventricle and the inferior wall of the left ventricle. Blockages here are possible too, but the left main is more commonly associated with the symptoms described, especially when troponin is elevated.\n",
      "\n",
      "So putting it all together, the most likely coronary artery involved is the left main coronary artery. The combination of the patient's history, the elevated troponin, and the typical chest pain radiation points to this artery being the culprit.\n",
      "</think>\n",
      "\n",
      "The most likely coronary artery involved in this presentation is the **left main coronary artery (LMCA)**. \n",
      "\n",
      "**Explanation:**\n",
      "- **Symptoms:** The patient's sudden chest pain radiating to the neck and left arm, along with elevated troponin levels, suggests an acute coronary syndrome, likely a myocardial infarction (heart attack).\n",
      "- **Past Medical History:** History of hypercholesterolemia and coronary artery disease are risk factors for atherosclerosis, which can lead to blockages in the coronary arteries.\n",
      "- **Tachycardia:** Increased heart rate may occur as the heart compensates for reduced blood flow to the heart muscle.\n",
      "- **Coronary Artery Consideration:** The left main coronary artery supplies the left ventricle, a large muscle that is crucial for cardiac function. Blockages in the LMCA can lead to more severe and life-threatening heart attacks compared to blockages in the right coronary artery, which typically supply less critical areas.\n",
      "\n",
      "Thus, the combination of symptoms, elevated troponin, and the patient's history strongly points to the **left main coronary artery** as the most likely culprit.<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": "print(response2[0].split(\"### Response:\")[1])"
  },
  {
   "cell_type": "markdown",
   "id": "f6f13f64-11bc-47bb-8994-e1ccea729bb9",
   "metadata": {},
   "source": "ç¿»è¯‘å¦‚ä¸‹ï¼š\n\n\\<think>  \nå¥½çš„ï¼Œæˆ‘æœ‰ä¸€ä½æ‚£è€…ï¼Œçªç„¶å‡ºç°èƒ¸ç—›ï¼Œå¹¶æ”¾å°„åˆ°é¢ˆéƒ¨å’Œå·¦è‡‚ã€‚è¿™è®©æˆ‘æƒ³åˆ°äº†å¿ƒè„ç—…å‘ä½œï¼Œå› ä¸ºè¿™äº›ç—‡çŠ¶å¾ˆç»å…¸â€”â€”å¿ƒç»ç—›æˆ–å¿ƒè‚Œæ¢—æ­»ã€‚å·¦è‡‚ç—›ã€é¢ˆéƒ¨ç—›ï¼Œæœ‰æ—¶è¿˜ä¼šä¼´éšä¸‹é¢Œæˆ–èƒŒéƒ¨çš„ç–¼ç—›ï¼Œè¿™äº›éƒ½å¯èƒ½ä¸å† çŠ¶åŠ¨è„‰é—®é¢˜ç›¸å…³ã€‚\n\nä»ç—…å²æ¥çœ‹ï¼Œæ‚£è€…æœ‰**é«˜èƒ†å›ºé†‡è¡€ç—‡**ï¼ˆå³é«˜èƒ†å›ºé†‡ï¼‰å’Œ**å† çŠ¶åŠ¨è„‰ç–¾ç—…**ï¼Œè¿™ä¸¤ä¸ªå› ç´ éƒ½æ˜¯**åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–**çš„é£é™©å› ç´ ï¼Œå¯èƒ½å¯¼è‡´å† çŠ¶åŠ¨è„‰å‘ç”Ÿå µå¡ã€‚**è‚Œé’™è›‹ç™½Iå‡é«˜**æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„çº¿ç´¢ï¼Œå› ä¸ºè‚Œé’™è›‹ç™½æ˜¯å¿ƒè‚Œå—æŸæ—¶é‡Šæ”¾çš„å¿ƒè„é…¶ï¼Œé€šå¸¸è¡¨æ˜å‘ç”Ÿäº†å¿ƒè‚Œæ¢—æ­»ã€‚å¦å¤–ï¼Œæ‚£è€…è¿˜å‡ºç°äº†**å¿ƒåŠ¨è¿‡é€Ÿ**ï¼Œå³å¿ƒè·³æ¯”å¹³å¸¸å¿«ã€‚åœ¨å¿ƒè‚Œæ¢—æ­»æ—¶ï¼Œå¿ƒè„å¯èƒ½ä¼šåŠ é€Ÿè·³åŠ¨ï¼Œä»¥è¯•å›¾é€šè¿‡å¢åŠ å¿ƒè„è¾“å‡ºé‡æ¥è¡¥å¿è¢«é˜»å¡çš„å† çŠ¶åŠ¨è„‰ã€‚\n\nè€ƒè™‘åˆ°å† çŠ¶åŠ¨è„‰ï¼Œ**å·¦ä¸»å† çŠ¶åŠ¨è„‰**ï¼ˆLMCAï¼‰ä¸ºæ•´ä¸ªå·¦ä¾§å¿ƒè„æä¾›è¡€æ¶²ï¼ŒåŒ…æ‹¬å·¦å¿ƒå®¤ï¼Œè€Œå·¦å¿ƒå®¤æ˜¯ä¸€ä¸ªå…³é”®çš„æ³µè¡€è‚Œè‚‰ã€‚å¦‚æœè¿™é‡Œå‘ç”Ÿå µå¡ï¼Œå¯èƒ½å¯¼è‡´æ›´ä¸¥é‡çš„å¿ƒè‚Œæ¢—æ­»ï¼Œå› ä¸ºå·¦å¿ƒå®¤è‡³å…³é‡è¦ã€‚**å³å† çŠ¶åŠ¨è„‰**ä¸ºå³å¿ƒå®¤å’Œå·¦å¿ƒå®¤ä¸‹å£æä¾›è¡€æ¶²ï¼Œè¿™é‡Œçš„å µå¡ä¹Ÿæ˜¯å¯èƒ½çš„ï¼Œä½†**å·¦ä¸»å† çŠ¶åŠ¨è„‰**é€šå¸¸ä¸ä¸Šè¿°ç—‡çŠ¶æ›´ç›¸å…³ï¼Œå°¤å…¶æ˜¯å½“è‚Œé’™è›‹ç™½å‡é«˜æ—¶ã€‚\n\næ‰€ä»¥ï¼Œå°†æ‰€æœ‰å› ç´ ç»¼åˆè€ƒè™‘ï¼Œæœ€å¯èƒ½å—ç´¯çš„å† çŠ¶åŠ¨è„‰æ˜¯**å·¦ä¸»å† çŠ¶åŠ¨è„‰**ï¼ˆLMCAï¼‰ã€‚æ‚£è€…çš„ç—…å²ã€è‚Œé’™è›‹ç™½å‡é«˜ä»¥åŠå…¸å‹çš„èƒ¸ç—›æ”¾å°„ç—‡çŠ¶éƒ½æŒ‡å‘äº†è¿™ä¸€åŠ¨è„‰ä½œä¸ºç½ªé­ç¥¸é¦–ã€‚\n\n\\</think>\n\næœ€å¯èƒ½å—ç´¯çš„å† çŠ¶åŠ¨è„‰æ˜¯**å·¦ä¸»å† çŠ¶åŠ¨è„‰ï¼ˆLMCAï¼‰**ã€‚\n\n**è§£é‡Šï¼š**\n- **ç—‡çŠ¶ï¼š** æ‚£è€…çªå‘èƒ¸ç—›å¹¶æ”¾å°„è‡³é¢ˆéƒ¨å’Œå·¦è‡‚ï¼Œä»¥åŠè‚Œé’™è›‹ç™½å‡é«˜ï¼Œæç¤ºæ€¥æ€§å† çŠ¶åŠ¨è„‰ç»¼åˆç—‡ï¼Œå¯èƒ½æ˜¯å¿ƒè‚Œæ¢—æ­»ã€‚\n- **ç—…å²ï¼š** é«˜èƒ†å›ºé†‡è¡€ç—‡å’Œå† çŠ¶åŠ¨è„‰ç–¾ç—…ç—…å²æ˜¯åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–çš„é£é™©å› ç´ ï¼Œå¯èƒ½å¯¼è‡´å† çŠ¶åŠ¨è„‰å µå¡ã€‚\n- **å¿ƒåŠ¨è¿‡é€Ÿï¼š** å¿ƒç‡å¢åŠ å¯èƒ½æ˜¯å¿ƒè„ä¸ºè¡¥å¿å¿ƒè‚Œè¡€æµå‡å°‘è€Œäº§ç”Ÿçš„ååº”ã€‚\n- **å† çŠ¶åŠ¨è„‰è€ƒè™‘ï¼š** å·¦ä¸»å† çŠ¶åŠ¨è„‰ä¾›åº”å·¦å¿ƒå®¤ï¼Œè¿™ä¸ªè‚Œè‚‰å¯¹å¿ƒè„åŠŸèƒ½è‡³å…³é‡è¦ã€‚ä¸å³å† çŠ¶åŠ¨è„‰ç›¸æ¯”ï¼Œå·¦ä¸»å† çŠ¶åŠ¨è„‰çš„å µå¡ä¼šå¯¼è‡´æ›´ä¸¥é‡ä¸”å±åŠç”Ÿå‘½çš„å¿ƒè‚Œæ¢—æ­»ï¼Œå³å† çŠ¶åŠ¨è„‰é€šå¸¸ä¾›åº”çš„æ˜¯ä¸é‚£ä¹ˆå…³é”®çš„åŒºåŸŸã€‚\n\nå› æ­¤ï¼Œç—‡çŠ¶ã€è‚Œé’™è›‹ç™½å‡é«˜ä»¥åŠæ‚£è€…çš„ç—…å²å¼ºçƒˆæŒ‡å‘**å·¦ä¸»å† çŠ¶åŠ¨è„‰**ï¼ˆLMCAï¼‰ä½œä¸ºæœ€å¯èƒ½çš„ç½ªé­ç¥¸é¦–ã€‚<ï½œendâ–ofâ–sentenceï½œ>"
  },
  {
   "cell_type": "markdown",
   "id": "d5acf498-88f5-47b7-831e-2dcbcb96e589",
   "metadata": {},
   "source": "æ ‡å‡†ç­”æ¡ˆï¼š\n\næ ¹æ®æ‚£è€…è¡¨ç°å‡ºçš„çªç„¶èƒ¸ç—›å¹¶æ”¾å°„è‡³é¢ˆéƒ¨å’Œå·¦è‡‚ï¼Œç»“åˆå…¶æœ‰é«˜èƒ†å›ºé†‡è¡€ç—‡å’Œå† çŠ¶åŠ¨è„‰ç–¾ç—…çš„ç—…å²ï¼Œè‚Œé’™è›‹ç™½å‡é«˜å’Œå¿ƒåŠ¨è¿‡é€Ÿï¼Œä¸´åºŠç—‡çŠ¶å¼ºçƒˆæç¤ºå·¦å‰é™æ”¯ï¼ˆLADï¼‰åŠ¨è„‰å—ç´¯ã€‚è¯¥åŠ¨è„‰é€šå¸¸æ˜¯å¼•å‘æ­¤ç±»ç—‡çŠ¶çš„ç½ªé­ç¥¸é¦–ï¼Œå› ä¸ºå®ƒä¾›åº”äº†å¿ƒè„çš„å¤§éƒ¨åˆ†åŒºåŸŸã€‚æ”¾å°„æ€§ç–¼ç—›å’Œè‚Œé’™è›‹ç™½å‡é«˜çš„ç»„åˆè¡¨æ˜å¿ƒè‚Œå—æŸï¼Œè¿™ä½¿å¾—LADæˆä¸ºæœ€å¯èƒ½çš„è‡´ç—…åŠ¨è„‰ã€‚ç„¶è€Œï¼Œåœ¨æ²¡æœ‰è¿›ä¸€æ­¥çš„è¯Šæ–­æ£€æŸ¥ï¼ˆå¦‚å¿ƒç”µå›¾ï¼‰çš„æƒ…å†µä¸‹ï¼Œæœ€ç»ˆçš„ç¡®è¯Šä»éœ€ç­‰å¾…ç¡®è®¤ã€‚"
  },
  {
   "cell_type": "markdown",
   "id": "1e7cdfee-2b25-4fdc-9ecd-ac5ea4a30e41",
   "metadata": {},
   "source": "èƒ½å¤Ÿçœ‹å‡ºï¼Œåœ¨åŸå§‹çŠ¶æ€ä¸‹ï¼Œæ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæ¨ç†å¹¶ç»™å‡ºå›å¤ï¼Œä½†å®é™…ä¸Šç¬¬ä¸€ä¸ªå›ç­”è¿‡ç¨‹å¹¶ä¸ç¬¦åˆåŒ»å­¦è§„èŒƒï¼Œè€Œç¬¬äºŒä¸ªé—®é¢˜åˆ™ç›´æ¥å›ç­”é”™è¯¯ã€‚ç”±æ­¤å¯è§ï¼Œåœ¨åˆå§‹çŠ¶æ€ä¸‹ï¼Œæ¨¡å‹å¯¹äºmedical-o1-reasoning-SFTæ•°æ®é›†é—®ç­”æ•ˆæœå¹¶ä¸å¥½ã€‚"
  },
  {
   "cell_type": "markdown",
   "id": "3ec2dc95-c4c8-4a03-acb9-b8c5bce92e9d",
   "metadata": {},
   "source": "&emsp;&emsp;æ¥ä¸‹æ¥å°è¯•è¿›è¡Œå¾®è°ƒï¼Œå¹¶æµ‹è¯•å¾®è°ƒåæ¨¡å‹é—®ç­”æ•ˆæœã€‚"
  },
  {
   "cell_type": "markdown",
   "id": "e27e5d96-c090-4d06-b133-c36b789e41b8",
   "metadata": {},
   "source": "### äºŒã€æœ€å°å¯è¡Œæ€§å®éªŒ"
  },
  {
   "cell_type": "markdown",
   "id": "e6842985-e49e-44f3-9fa5-2e2ba3ec4a3b",
   "metadata": {},
   "source": "&emsp;&emsp;æ¥ä¸‹æ¥æˆ‘ä»¬å°è¯•è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œå¯¹äºå½“å‰æ•°æ®é›†è€Œè¨€ï¼Œæˆ‘ä»¬å¯ä»¥å¸¦å…¥åŸå§‹æ•°æ®é›†çš„éƒ¨åˆ†æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä¹Ÿå¯ä»¥å¸¦å…¥å…¨éƒ¨æ•°æ®å¹¶éå†å¤šæ¬¡è¿›è¡Œå¾®è°ƒã€‚å¯¹äºå¤§å¤šæ•°çš„å¾®è°ƒå®éªŒï¼Œæˆ‘ä»¬éƒ½å¯ä»¥ä»æœ€å°å¯è¡Œæ€§å®éªŒå…¥æ‰‹è¿›è¡Œå¾®è°ƒï¼Œä¹Ÿå°±æ˜¯å…ˆå°è¯•å¸¦å…¥å°‘é‡æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå¹¶è§‚æµ‹å¾®è°ƒæ•ˆæœã€‚è‹¥å¾®è°ƒå¯ä»¥é¡ºåˆ©æ‰§è¡Œï¼Œå¹¶èƒ½å¤Ÿè·å¾—å¾®è°ƒæ•ˆæœï¼Œå†è€ƒè™‘å¸¦å…¥æ›´å¤šçš„æ•°æ®è¿›è¡Œæ›´å¤§è§„æ¨¡å¾®è°ƒã€‚"
  },
  {
   "cell_type": "markdown",
   "id": "6672971b-0d59-4c5a-9461-fa7e4c8e1cb4",
   "metadata": {},
   "source": "#### 1.æ•°æ®é›†å‡†å¤‡"
  },
  {
   "cell_type": "markdown",
   "id": "441dc132-747d-40ec-9269-f315678b29e0",
   "metadata": {},
   "source": "&emsp;&emsp;è¿™é‡Œæˆ‘ä»¬ç›´æ¥ä»huggingfaceä¸Šä¸‹è½½medical-o1-reasoning-SFTæ•°æ®é›†ã€‚"
  },
  {
   "cell_type": "markdown",
   "id": "7ed0b548-4e41-406b-8d52-110509156eb9",
   "metadata": {},
   "source": "- è®¾ç½®ä»£ç†ç¯å¢ƒ"
  },
  {
   "cell_type": "markdown",
   "id": "f36ac067-8d97-4999-9a3e-1ef305751f39",
   "metadata": {},
   "source": "&emsp;&emsp;ç”±äºhuggingfaceç½‘ç»œå—é™ï¼Œä¸‹è½½æ•°æ®é›†å‰éœ€è¦å…ˆè¿›è¡Œç½‘ç»œç¯å¢ƒè®¾ç½®ã€‚è‹¥æ˜¯AutoDLæœåŠ¡å™¨ï¼Œåˆ™å¯ä»¥æŒ‰ç…§å¦‚ä¸‹æ–¹å¼å¼€å¯å­¦æœ¯åŠ é€Ÿï¼Œä»è€Œé¡ºåˆ©è¿æ¥huggingfaceå¹¶è¿›è¡Œæ•°æ®é›†ä¸‹è½½ï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "281d6b32-4b56-4184-80da-202d7dd589af",
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport os\n\nresult = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\noutput = result.stdout\nfor line in output.splitlines():\n    if '=' in line:\n        var, value = line.split('=', 1)\n        os.environ[var] = value"
  },
  {
   "cell_type": "markdown",
   "id": "baef1f84-74f4-4779-9326-a11502aa6ae1",
   "metadata": {},
   "source": "- ä¸‹è½½æ•°æ®é›†"
  },
  {
   "cell_type": "markdown",
   "id": "bf0d4f6f-38ce-438e-be64-951f4b907fd7",
   "metadata": {},
   "source": "&emsp;&emsp;æ¥ä¸‹æ¥ä½¿ç”¨datasetsè¿›è¡Œæ•°æ®é›†ä¸‹è½½"
  },
  {
   "cell_type": "code",
   "id": "5924a688-b61e-4d6d-bc2e-fa0b8c013ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T13:20:21.859810Z",
     "start_time": "2025-02-16T13:20:21.855089Z"
    }
   },
   "source": "# !pip install datasets",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "fd4635bd-5d5d-406b-900d-eccbfaf261a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:32:58.062409Z",
     "start_time": "2025-02-18T08:32:58.059181Z"
    }
   },
   "source": "import os\nfrom datasets import load_dataset\n",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "b90685ac-6510-4fd4-9c2d-def17e0dc6d5",
   "metadata": {},
   "source": "å†æ¬¡ç¡®è®¤æç¤ºè¯æ¨¡æ¿ï¼š"
  },
  {
   "cell_type": "code",
   "id": "acef18a3-d669-4acf-8da7-4da22b3aaee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:33:00.346683Z",
     "start_time": "2025-02-18T08:33:00.343942Z"
    }
   },
   "source": "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "e53db946-57c9-40c0-aded-1ac2462f12ec",
   "metadata": {},
   "source": "ç„¶åæå–å¹¶è®¾ç½®æ–‡æœ¬ç”Ÿæˆç»“æŸçš„æ ‡è®°ï¼š"
  },
  {
   "cell_type": "code",
   "id": "05c75cc9-61f4-4ff3-9124-34ff78456a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:33:03.063924Z",
     "start_time": "2025-02-18T08:33:03.060434Z"
    }
   },
   "source": "EOS_TOKEN = tokenizer.eos_token\ntokenizer.eos_token",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<ï½œendâ–ofâ–sentenceï½œ>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "0117c824-4486-4b17-bbd0-cb3ff9a34f4d",
   "metadata": {},
   "source": "ç„¶åå®šä¹‰å‡½æ•°ï¼Œç”¨äºå¯¹medical-o1-reasoning-SFTæ•°æ®é›†è¿›è¡Œä¿®æ”¹ï¼ŒComplex_CoTåˆ—å’ŒResponseåˆ—è¿›è¡Œæ‹¼æ¥ï¼Œå¹¶åŠ ä¸Šæ–‡æœ¬ç»“æŸæ ‡è®°ï¼š"
  },
  {
   "cell_type": "code",
   "id": "9dee28cc-1fe8-4c79-860b-c7792b269530",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:33:04.895361Z",
     "start_time": "2025-02-18T08:33:04.892409Z"
    }
   },
   "source": "def formatting_prompts_func(examples):\n    inputs = examples[\"Question\"]\n    cots = examples[\"Complex_CoT\"]\n    outputs = examples[\"Response\"]\n    texts = []\n    for input, cot, output in zip(inputs, cots, outputs):\n        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n        texts.append(text)\n    return {\n        \"text\": texts,\n    }",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "14b85065-f6a9-4bff-ae58-cdd940bb2cbf",
   "metadata": {},
   "source": "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206180316919.png\" alt=\"image-20250206180316919\" style=\"zoom:50%;\" />"
  },
  {
   "cell_type": "markdown",
   "id": "c4456066-89d8-4ad0-9a2c-edf72eca77f9",
   "metadata": {},
   "source": "åœ¨æœ€å°å¯è¡Œæ€§å®éªŒä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åªä¸‹è½½500æ¡æ•°æ®è¿›è¡Œå¾®è°ƒå³å¯çœ‹å‡ºæ•ˆæœï¼š"
  },
  {
   "cell_type": "code",
   "id": "c18961a1-0db6-404f-81ad-182b2b62a3c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:33:13.621127Z",
     "start_time": "2025-02-18T08:33:08.394931Z"
    }
   },
   "source": "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"zh\", split = \"train[0:500]\",trust_remote_code=True)",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "85106a25-688e-447f-b411-11cc1b0206df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:33:13.628832Z",
     "start_time": "2025-02-18T08:33:13.625122Z"
    }
   },
   "source": "dataset[0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'æ ¹æ®æè¿°ï¼Œä¸€ä¸ª1å²çš„å­©å­åœ¨å¤å­£å¤´çš®å‡ºç°å¤šå¤„å°ç»“èŠ‚ï¼Œé•¿æœŸä¸æ„ˆåˆï¼Œä¸”ç°åœ¨ç–®å¤§å¦‚æ¢…ï¼Œæºƒç ´æµè„“ï¼Œå£ä¸æ”¶æ•›ï¼Œå¤´çš®ä¸‹æœ‰ç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšã€‚è¿™ç§ç—…ç—‡åœ¨ä¸­åŒ»ä¸­è¯Šæ–­ä¸ºä»€ä¹ˆç—…ï¼Ÿ',\n",
       " 'Complex_CoT': 'è¿™ä¸ªå°å­©å­åœ¨å¤å¤©å¤´çš®ä¸Šé•¿äº†äº›å°ç»“èŠ‚ï¼Œä¸€ç›´éƒ½æ²¡å¥½ï¼Œåæ¥å˜æˆäº†è„“åŒ…ï¼Œæµäº†å¥½å¤šè„“ã€‚æƒ³æƒ³å¤å¤©é‚£ä¹ˆçƒ­ï¼Œå¯èƒ½å’Œæ¹¿çƒ­æœ‰å…³ã€‚æ‰ä¸€å²çš„å°å­©ï¼Œå…ç–«åŠ›æœ¬æ¥å°±ä¸å¼ºï¼Œå¤å¤©çš„æ¹¿çƒ­æ²¡å‡†å°±ä¾µè¢­äº†èº«ä½“ã€‚\\n\\nç”¨ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œå‡ºç°å°ç»“èŠ‚ã€å†åŠ ä¸Šé•¿æœŸä¸æ„ˆåˆï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°äº†å¤´ç–®ã€‚å°å­©å­æœ€å®¹æ˜“å¾—è¿™äº›çš®è‚¤ç—…ï¼Œä¸»è¦å› ä¸ºæ¹¿çƒ­åœ¨ä½“è¡¨éƒç»“ã€‚\\n\\nä½†å†çœ‹çœ‹ï¼Œå¤´çš®ä¸‹è¿˜æœ‰ç©ºæ´ï¼Œè¿™å¯èƒ½ä¸æ­¢æ˜¯ç®€å•çš„å¤´ç–®ã€‚çœ‹èµ·æ¥ç—…æƒ…æŒºä¸¥é‡çš„ï¼Œä¹Ÿè®¸æ˜¯è„“è‚¿æ²¡æ²»å¥½ã€‚è¿™æ ·çš„æƒ…å†µä¸­åŒ»ä¸­æœ‰æ—¶å€™å«åšç¦¿ç–®æˆ–è€…æ¹¿ç–®ï¼Œä¹Ÿå¯èƒ½æ˜¯å¦ä¸€ç§æƒ…å†µã€‚\\n\\nç­‰ä¸€ä¸‹ï¼Œå¤´çš®ä¸Šçš„ç©ºæ´å’Œçš®è‚¤å¢åšæ›´åƒæ˜¯ç–¾ç—…å·²ç»æ·±å…¥åˆ°å¤´çš®ä¸‹ï¼Œè¿™æ˜¯ä¸æ˜¯è¯´æ˜æœ‰å¯èƒ½æ˜¯æµæ³¨æˆ–ç˜°ç–¬ï¼Ÿè¿™äº›åå­—å¸¸æè¿°å¤´éƒ¨æˆ–é¢ˆéƒ¨çš„ä¸¥é‡æ„ŸæŸ“ï¼Œç‰¹åˆ«æ˜¯æœ‰åŒ–è„“ä¸æ„ˆåˆï¼Œåˆå½¢æˆé€šé“æˆ–ç©ºæ´çš„æƒ…å†µã€‚\\n\\nä»”ç»†æƒ³æƒ³ï¼Œæˆ‘æ€ä¹ˆæ„Ÿè§‰è¿™äº›ç—‡çŠ¶æ›´è´´è¿‘ç˜°ç–¬çš„è¡¨ç°ï¼Ÿå°¤å…¶è€ƒè™‘åˆ°å­©å­çš„å¹´çºªå’Œå¤å¤©å‘ç”Ÿçš„å­£èŠ‚æ€§å› ç´ ï¼Œæ¹¿çƒ­å¯èƒ½æ˜¯ä¸»å› ï¼Œä½†å¯èƒ½ä¹Ÿæœ‰ç«æ¯’æˆ–è€…ç—°æ¹¿é€ æˆçš„æ»ç•™ã€‚\\n\\nå›åˆ°åŸºæœ¬çš„ç—‡çŠ¶æè¿°ä¸Šçœ‹ï¼Œè¿™ç§é•¿æœŸä¸æ„ˆåˆåˆå¤æ‚çš„çŠ¶å†µï¼Œå¦‚æœç»“åˆä¸­åŒ»æ›´åé‡çš„ç—…åï¼Œæ˜¯ä¸æ˜¯æœ‰å¯èƒ½æ˜¯æ¶‰åŠæ›´æ·±å±‚æ¬¡çš„æ„ŸæŸ“ï¼Ÿ\\n\\nå†è€ƒè™‘ä¸€ä¸‹ï¼Œè¿™åº”è¯¥ä¸æ˜¯å•çº¯çš„ç˜°ç–¬ï¼Œå¾—ä»”ç»†åˆ†æå¤´çš®å¢åšå¹¶å‡ºç°ç©ºæ´è¿™æ ·çš„ä¸¥é‡ç—‡çŠ¶ã€‚ä¸­åŒ»é‡Œå¤´ï¼Œè¿™æ ·çš„è¡¨ç°å¯èƒ½æ›´ç¬¦åˆâ€˜èš€ç–®â€™æˆ–â€˜å¤´ç–½â€™ã€‚è¿™äº›ç—…åé€šå¸¸æè¿°å¤´éƒ¨ä¸¥é‡æ„ŸæŸ“åçš„æºƒçƒ‚å’Œç»„ç»‡åæ­»ã€‚\\n\\nçœ‹çœ‹å­£èŠ‚å’Œå­©å­çš„ä½“è´¨ï¼Œå¤å¤©åˆæ¹¿åˆçƒ­ï¼Œå¤–é‚ªå¾ˆå®¹æ˜“ä¾µå…¥å¤´éƒ¨ï¼Œå¯¹å­©å­è¿™ä¹ˆå¼±çš„å…ç–«ç³»ç»Ÿç®€ç›´å°±æ˜¯æŒ‘æˆ˜ã€‚å¤´ç–½è¿™ä¸ªç—…åå¬èµ·æ¥çœŸæ˜¯åˆ‡åˆï¼Œå› ä¸ºå®ƒæè¿°çš„æ„ŸæŸ“ä¸¥é‡ï¼Œæºƒçƒ‚åˆ°å‡ºç°ç©ºæ´ã€‚\\n\\nä¸è¿‡ï¼Œä»”ç»†ç¢ç£¨åå‘ç°ï¼Œè¿˜æœ‰ä¸ªç—…åä¼¼ä¹æ›´ä¸ºåˆé€‚ï¼Œå«åšâ€˜è¼è›„ç––â€™ï¼Œè¿™ç—…åœ¨ä¸­åŒ»é‡Œä¸“æŒ‡åƒè¿™ç§ä¸¥é‡æ„ŸæŸ“å¹¶ä¼´æœ‰æ·±éƒ¨ç©ºæ´çš„æƒ…å†µã€‚å®ƒä¹Ÿæ¶µç›–äº†åŒ–è„“å’Œçš®è‚¤å¢åšè¿™äº›ç—‡çŠ¶ã€‚\\n\\nå“¦ï¼Œè¯¥ä¸ä¼šæ˜¯å¤å­£æ¹¿çƒ­ï¼Œå¯¼è‡´æ¹¿æ¯’å…¥ä¾µï¼Œå­©å­çš„ä½“è´¨ä¸èƒ½å¾¡ï¼Œå…¶ç—…æƒ…å‘å±•æˆè¿™æ ·çš„æ„ŸæŸ“ï¼Ÿç»¼åˆåˆ†æåæˆ‘è§‰å¾—â€˜è¼è›„ç––â€™è¿™ä¸ªç—…åçœŸæ˜¯ç›¸å½“ç¬¦åˆã€‚',\n",
       " 'Response': 'ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "7fca9256-067a-4f7b-a339-e0473bc4a81e",
   "metadata": {},
   "source": "ç„¶åè¿›è¡Œç»“æ„åŒ–å¤„ç†ï¼š"
  },
  {
   "cell_type": "code",
   "id": "82c249c5-ec25-448d-8045-46042d9ad963",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:33:13.723206Z",
     "start_time": "2025-02-18T08:33:13.715865Z"
    }
   },
   "source": "dataset = dataset.map(formatting_prompts_func, batched = True,)",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "24fc62f8-96f2-4905-b0e6-c28a60954ce7",
   "metadata": {},
   "source": "å°†æ•°æ®é›†æ•´ç†ä¸ºå¦‚ä¸‹å½¢å¼ï¼š"
  },
  {
   "cell_type": "code",
   "id": "1f34f952-656e-44d6-a778-8a70690d4c94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:33:13.760798Z",
     "start_time": "2025-02-18T08:33:13.756255Z"
    }
   },
   "source": "dataset[\"text\"][0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task, paired with an input that provides further context.\\nWrite a response that appropriately completes the request.\\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\\nPlease answer the following medical question.\\n\\n### Question:\\næ ¹æ®æè¿°ï¼Œä¸€ä¸ª1å²çš„å­©å­åœ¨å¤å­£å¤´çš®å‡ºç°å¤šå¤„å°ç»“èŠ‚ï¼Œé•¿æœŸä¸æ„ˆåˆï¼Œä¸”ç°åœ¨ç–®å¤§å¦‚æ¢…ï¼Œæºƒç ´æµè„“ï¼Œå£ä¸æ”¶æ•›ï¼Œå¤´çš®ä¸‹æœ‰ç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšã€‚è¿™ç§ç—…ç—‡åœ¨ä¸­åŒ»ä¸­è¯Šæ–­ä¸ºä»€ä¹ˆç—…ï¼Ÿ\\n\\n### Response:\\n<think>\\nè¿™ä¸ªå°å­©å­åœ¨å¤å¤©å¤´çš®ä¸Šé•¿äº†äº›å°ç»“èŠ‚ï¼Œä¸€ç›´éƒ½æ²¡å¥½ï¼Œåæ¥å˜æˆäº†è„“åŒ…ï¼Œæµäº†å¥½å¤šè„“ã€‚æƒ³æƒ³å¤å¤©é‚£ä¹ˆçƒ­ï¼Œå¯èƒ½å’Œæ¹¿çƒ­æœ‰å…³ã€‚æ‰ä¸€å²çš„å°å­©ï¼Œå…ç–«åŠ›æœ¬æ¥å°±ä¸å¼ºï¼Œå¤å¤©çš„æ¹¿çƒ­æ²¡å‡†å°±ä¾µè¢­äº†èº«ä½“ã€‚\\n\\nç”¨ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œå‡ºç°å°ç»“èŠ‚ã€å†åŠ ä¸Šé•¿æœŸä¸æ„ˆåˆï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°äº†å¤´ç–®ã€‚å°å­©å­æœ€å®¹æ˜“å¾—è¿™äº›çš®è‚¤ç—…ï¼Œä¸»è¦å› ä¸ºæ¹¿çƒ­åœ¨ä½“è¡¨éƒç»“ã€‚\\n\\nä½†å†çœ‹çœ‹ï¼Œå¤´çš®ä¸‹è¿˜æœ‰ç©ºæ´ï¼Œè¿™å¯èƒ½ä¸æ­¢æ˜¯ç®€å•çš„å¤´ç–®ã€‚çœ‹èµ·æ¥ç—…æƒ…æŒºä¸¥é‡çš„ï¼Œä¹Ÿè®¸æ˜¯è„“è‚¿æ²¡æ²»å¥½ã€‚è¿™æ ·çš„æƒ…å†µä¸­åŒ»ä¸­æœ‰æ—¶å€™å«åšç¦¿ç–®æˆ–è€…æ¹¿ç–®ï¼Œä¹Ÿå¯èƒ½æ˜¯å¦ä¸€ç§æƒ…å†µã€‚\\n\\nç­‰ä¸€ä¸‹ï¼Œå¤´çš®ä¸Šçš„ç©ºæ´å’Œçš®è‚¤å¢åšæ›´åƒæ˜¯ç–¾ç—…å·²ç»æ·±å…¥åˆ°å¤´çš®ä¸‹ï¼Œè¿™æ˜¯ä¸æ˜¯è¯´æ˜æœ‰å¯èƒ½æ˜¯æµæ³¨æˆ–ç˜°ç–¬ï¼Ÿè¿™äº›åå­—å¸¸æè¿°å¤´éƒ¨æˆ–é¢ˆéƒ¨çš„ä¸¥é‡æ„ŸæŸ“ï¼Œç‰¹åˆ«æ˜¯æœ‰åŒ–è„“ä¸æ„ˆåˆï¼Œåˆå½¢æˆé€šé“æˆ–ç©ºæ´çš„æƒ…å†µã€‚\\n\\nä»”ç»†æƒ³æƒ³ï¼Œæˆ‘æ€ä¹ˆæ„Ÿè§‰è¿™äº›ç—‡çŠ¶æ›´è´´è¿‘ç˜°ç–¬çš„è¡¨ç°ï¼Ÿå°¤å…¶è€ƒè™‘åˆ°å­©å­çš„å¹´çºªå’Œå¤å¤©å‘ç”Ÿçš„å­£èŠ‚æ€§å› ç´ ï¼Œæ¹¿çƒ­å¯èƒ½æ˜¯ä¸»å› ï¼Œä½†å¯èƒ½ä¹Ÿæœ‰ç«æ¯’æˆ–è€…ç—°æ¹¿é€ æˆçš„æ»ç•™ã€‚\\n\\nå›åˆ°åŸºæœ¬çš„ç—‡çŠ¶æè¿°ä¸Šçœ‹ï¼Œè¿™ç§é•¿æœŸä¸æ„ˆåˆåˆå¤æ‚çš„çŠ¶å†µï¼Œå¦‚æœç»“åˆä¸­åŒ»æ›´åé‡çš„ç—…åï¼Œæ˜¯ä¸æ˜¯æœ‰å¯èƒ½æ˜¯æ¶‰åŠæ›´æ·±å±‚æ¬¡çš„æ„ŸæŸ“ï¼Ÿ\\n\\nå†è€ƒè™‘ä¸€ä¸‹ï¼Œè¿™åº”è¯¥ä¸æ˜¯å•çº¯çš„ç˜°ç–¬ï¼Œå¾—ä»”ç»†åˆ†æå¤´çš®å¢åšå¹¶å‡ºç°ç©ºæ´è¿™æ ·çš„ä¸¥é‡ç—‡çŠ¶ã€‚ä¸­åŒ»é‡Œå¤´ï¼Œè¿™æ ·çš„è¡¨ç°å¯èƒ½æ›´ç¬¦åˆâ€˜èš€ç–®â€™æˆ–â€˜å¤´ç–½â€™ã€‚è¿™äº›ç—…åé€šå¸¸æè¿°å¤´éƒ¨ä¸¥é‡æ„ŸæŸ“åçš„æºƒçƒ‚å’Œç»„ç»‡åæ­»ã€‚\\n\\nçœ‹çœ‹å­£èŠ‚å’Œå­©å­çš„ä½“è´¨ï¼Œå¤å¤©åˆæ¹¿åˆçƒ­ï¼Œå¤–é‚ªå¾ˆå®¹æ˜“ä¾µå…¥å¤´éƒ¨ï¼Œå¯¹å­©å­è¿™ä¹ˆå¼±çš„å…ç–«ç³»ç»Ÿç®€ç›´å°±æ˜¯æŒ‘æˆ˜ã€‚å¤´ç–½è¿™ä¸ªç—…åå¬èµ·æ¥çœŸæ˜¯åˆ‡åˆï¼Œå› ä¸ºå®ƒæè¿°çš„æ„ŸæŸ“ä¸¥é‡ï¼Œæºƒçƒ‚åˆ°å‡ºç°ç©ºæ´ã€‚\\n\\nä¸è¿‡ï¼Œä»”ç»†ç¢ç£¨åå‘ç°ï¼Œè¿˜æœ‰ä¸ªç—…åä¼¼ä¹æ›´ä¸ºåˆé€‚ï¼Œå«åšâ€˜è¼è›„ç––â€™ï¼Œè¿™ç—…åœ¨ä¸­åŒ»é‡Œä¸“æŒ‡åƒè¿™ç§ä¸¥é‡æ„ŸæŸ“å¹¶ä¼´æœ‰æ·±éƒ¨ç©ºæ´çš„æƒ…å†µã€‚å®ƒä¹Ÿæ¶µç›–äº†åŒ–è„“å’Œçš®è‚¤å¢åšè¿™äº›ç—‡çŠ¶ã€‚\\n\\nå“¦ï¼Œè¯¥ä¸ä¼šæ˜¯å¤å­£æ¹¿çƒ­ï¼Œå¯¼è‡´æ¹¿æ¯’å…¥ä¾µï¼Œå­©å­çš„ä½“è´¨ä¸èƒ½å¾¡ï¼Œå…¶ç—…æƒ…å‘å±•æˆè¿™æ ·çš„æ„ŸæŸ“ï¼Ÿç»¼åˆåˆ†æåæˆ‘è§‰å¾—â€˜è¼è›„ç––â€™è¿™ä¸ªç—…åçœŸæ˜¯ç›¸å½“ç¬¦åˆã€‚\\n</think>\\nä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚<ï½œendâ–ofâ–sentenceï½œ>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "c478885f-5a4b-431c-b705-6faea5bcf087",
   "metadata": {},
   "source": "- æ•°æ®é›†ä¿å­˜åœ°å€"
  },
  {
   "cell_type": "markdown",
   "id": "304ca9f9-34ed-45d8-be58-b4ed829dbcbf",
   "metadata": {},
   "source": "é»˜è®¤æƒ…å†µä¸‹æ•°æ®é›†ä¿å­˜åœ¨ä¸»ç›®å½•ä¸‹.cacheæ–‡ä»¶å¤¹ä¸­ï¼Œæ•°æ®æ–‡ä»¶æ ¼å¼å¦‚ä¸‹æ‰€ç¤ºï¼š"
  },
  {
   "cell_type": "markdown",
   "id": "2f7e6e91-be9b-4fb7-90ba-7aac308bf293",
   "metadata": {},
   "source": "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206195216257.png\" alt=\"image-20250206195216257\" style=\"zoom:50%;\" />"
  },
  {
   "cell_type": "markdown",
   "id": "614a4418-2f7b-4b3e-9efa-efa2a5fd6dd7",
   "metadata": {},
   "source": "#### 2.å¼€å¯å¾®è°ƒ"
  },
  {
   "cell_type": "markdown",
   "id": "b6b7956b-039b-483f-9b50-15caf056dfb9",
   "metadata": {},
   "source": "&emsp;&emsp;ç„¶åå³å¯æŠŠæ¨¡å‹è®¾ç½®ä¸ºå¾®è°ƒæ¨¡å¼ï¼š"
  },
  {
   "cell_type": "code",
   "id": "028f9bf9-c089-4546-af8e-9eb56ca31b60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:33:19.504933Z",
     "start_time": "2025-02-18T08:33:17.801101Z"
    }
   },
   "source": "model = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  \n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    lora_alpha=16,\n    lora_dropout=0,  \n    bias=\"none\",  \n    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n    random_state=3407,\n    use_rslora=False,  \n    loftq_config=None,\n)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "732d5b06-50b4-4d78-a902-e2df87221f3f",
   "metadata": {},
   "source": "ç„¶åå¯¼å…¥ç›¸å…³çš„åº“ï¼š"
  },
  {
   "cell_type": "code",
   "id": "6d135473-1c60-4555-9a14-8abd5e5cabbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:33:21.509253Z",
     "start_time": "2025-02-18T08:33:21.506754Z"
    }
   },
   "source": "from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "23e168c5-df66-4a26-b65d-ad5251db7e76",
   "metadata": {},
   "source": "åˆ›å»ºæœ‰ç›‘ç£å¾®è°ƒå¯¹è±¡ï¼š"
  },
  {
   "cell_type": "code",
   "id": "1e1622f1-a0d9-495e-8162-8623027d4a00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:33:29.982352Z",
     "start_time": "2025-02-18T08:33:29.743734Z"
    }
   },
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
    "        warmup_steps=5,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\AI\\LLM\\unsloth_compiled_cache\\UnslothSFTTrainer.py:593: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "E:\\AI\\LLM\\unsloth_compiled_cache\\UnslothSFTTrainer.py:607: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "E:\\AI\\LLM\\unsloth_compiled_cache\\UnslothSFTTrainer.py:631: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "E:\\AI\\LLM\\unsloth_compiled_cache\\UnslothSFTTrainer.py:726: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `UnslothSFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "4836f8b5-b608-4af2-be5c-c1a4d4de74c5",
   "metadata": {},
   "source": "è¿™æ®µä»£ç ä¸»è¦æ˜¯ç”¨ **`SFTTrainer`** è¿›è¡Œ **ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuning, SFTï¼‰**ï¼Œé€‚ç”¨äº `transformers` å’Œ `Unsloth` ç”Ÿæ€ä¸­çš„æ¨¡å‹å¾®è°ƒï¼š\n**1. å¯¼å…¥ç›¸å…³åº“**\n- **`SFTTrainer`**ï¼ˆæ¥è‡ª `trl` åº“ï¼‰ï¼š  \n  - `trl`ï¼ˆTransformer Reinforcement Learningï¼‰æ˜¯ Hugging Face æ——ä¸‹çš„ `trl` åº“ï¼Œæä¾› **ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰** å’Œ **å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰** ç›¸å…³çš„åŠŸèƒ½ã€‚\n  - `SFTTrainer` ä¸»è¦ç”¨äº **æœ‰ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰**ï¼Œé€‚ç”¨äº `LoRA` ç­‰ä½ç§©é€‚é…å¾®è°ƒæ–¹å¼ã€‚\n\n- **`TrainingArguments`**ï¼ˆæ¥è‡ª `transformers` åº“ï¼‰ï¼š  \n  - è¿™ä¸ªç±»ç”¨äºå®šä¹‰ **è®­ç»ƒè¶…å‚æ•°**ï¼Œæ¯”å¦‚æ‰¹é‡å¤§å°ã€å­¦ä¹ ç‡ã€ä¼˜åŒ–å™¨ã€è®­ç»ƒæ­¥æ•°ç­‰ã€‚\n\n- **`is_bfloat16_supported()`**ï¼ˆæ¥è‡ª `unsloth`ï¼‰ï¼š  \n  - è¿™ä¸ªå‡½æ•°æ£€æŸ¥ **å½“å‰ GPU æ˜¯å¦æ”¯æŒ `bfloat16`ï¼ˆBF16ï¼‰**ï¼Œå¦‚æœæ”¯æŒï¼Œåˆ™è¿”å› `True`ï¼Œå¦åˆ™è¿”å› `False`ã€‚\n  - `bfloat16` æ˜¯ä¸€ç§æ›´é«˜æ•ˆçš„æ•°å€¼æ ¼å¼ï¼Œåœ¨ **æ–°æ¬¾ NVIDIA A100/H100** ç­‰ GPU ä¸Šè¡¨ç°æ›´ä¼˜ã€‚\n\n**2. åˆå§‹åŒ– `SFTTrainer` è¿›è¡Œæ¨¡å‹å¾®è°ƒ**\n\n##### **å‚æ•°è§£æ**\n##### **â‘  `SFTTrainer` éƒ¨åˆ†**\n| å‚æ•° | ä½œç”¨ |\n|------|------|\n| `model=model` | æŒ‡å®šéœ€è¦è¿›è¡Œå¾®è°ƒçš„ **é¢„è®­ç»ƒæ¨¡å‹** |\n| `tokenizer=tokenizer` | æŒ‡å®š **åˆ†è¯å™¨**ï¼Œç”¨äºå¤„ç†æ–‡æœ¬æ•°æ® |\n| `train_dataset=dataset` | ä¼ å…¥ **è®­ç»ƒæ•°æ®é›†** |\n| `dataset_text_field=\"text\"` | æŒ‡å®šæ•°æ®é›†ä¸­å“ªä¸€åˆ—åŒ…å« **è®­ç»ƒæ–‡æœ¬**ï¼ˆåœ¨ `formatting_prompts_func` é‡Œå¤„ç†ï¼‰ |\n| `max_seq_length=max_seq_length` | **æœ€å¤§åºåˆ—é•¿åº¦**ï¼Œæ§åˆ¶è¾“å…¥æ–‡æœ¬çš„æœ€å¤§ Token æ•°é‡ |\n| `dataset_num_proc=2` | **æ•°æ®åŠ è½½çš„å¹¶è¡Œè¿›ç¨‹æ•°**ï¼Œæé«˜æ•°æ®é¢„å¤„ç†æ•ˆç‡ |\n\n##### **â‘¡ `TrainingArguments` éƒ¨åˆ†**\n| å‚æ•° | ä½œç”¨ |\n|------|------|\n| `per_device_train_batch_size=2` | æ¯ä¸ª **GPU/è®¾å¤‡** çš„è®­ç»ƒæ‰¹é‡å¤§å°ï¼ˆè¾ƒå°å€¼é€‚åˆå¤§æ¨¡å‹ï¼‰ |\n| `gradient_accumulation_steps=4` | **æ¢¯åº¦ç´¯ç§¯æ­¥æ•°**ï¼ˆç›¸å½“äº `batch_size=2 Ã— 4 = 8`ï¼‰ |\n| `warmup_steps=5` | **é¢„çƒ­æ­¥æ•°**ï¼ˆåˆå§‹é˜¶æ®µå­¦ä¹ ç‡è¾ƒä½ï¼Œç„¶åé€æ­¥å‡é«˜ï¼‰ |\n| `max_steps=60` | **æœ€å¤§è®­ç»ƒæ­¥æ•°**ï¼ˆæ§åˆ¶è®­ç»ƒçš„æ€»æ­¥æ•°ï¼Œæ­¤å¤„æ€»å…±çº¦æ¶ˆè€—60*8=480æ¡æ•°æ®ï¼‰ |\n| `learning_rate=2e-4` | **å­¦ä¹ ç‡**ï¼ˆ`2e-4` = 0.0002ï¼Œæ§åˆ¶æƒé‡æ›´æ–°å¹…åº¦ï¼‰ |\n| `fp16=not is_bfloat16_supported()` | å¦‚æœ **GPU ä¸æ”¯æŒ `bfloat16`ï¼Œåˆ™ä½¿ç”¨ `fp16`ï¼ˆ16ä½æµ®ç‚¹æ•°ï¼‰** |\n| `bf16=is_bfloat16_supported()` | å¦‚æœ **GPU æ”¯æŒ `bfloat16`ï¼Œåˆ™å¯ç”¨ `bfloat16`ï¼ˆè®­ç»ƒæ›´ç¨³å®šï¼‰** |\n| `logging_steps=10` | **æ¯ 10 æ­¥è®°å½•ä¸€æ¬¡è®­ç»ƒæ—¥å¿—** |\n| `optim=\"adamw_8bit\"` | **ä½¿ç”¨ `adamw_8bit`ï¼ˆ8-bit AdamWä¼˜åŒ–å™¨ï¼‰å‡å°‘æ˜¾å­˜å ç”¨** |\n| `weight_decay=0.01` | **æƒé‡è¡°å‡ï¼ˆL2 æ­£åˆ™åŒ–ï¼‰**ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ |\n| `lr_scheduler_type=\"linear\"` | **å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥**ï¼ˆçº¿æ€§è¡°å‡ï¼‰ |\n| `seed=3407` | **éšæœºç§å­**ï¼ˆä¿è¯å®éªŒç»“æœå¯å¤ç°ï¼‰ |\n| `output_dir=\"outputs\"` | **è®­ç»ƒç»“æœçš„è¾“å‡ºç›®å½•** |"
  },
  {
   "cell_type": "markdown",
   "id": "153cc26d-f6c9-4979-bb12-9c07849bcf06",
   "metadata": {},
   "source": "ç„¶åè®¾ç½®wandbï¼ˆå¯é€‰ï¼‰ï¼š"
  },
  {
   "cell_type": "code",
   "id": "0fda0b4a-d2ca-4b36-87ca-59d025ef7a34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:33:36.432265Z",
     "start_time": "2025-02-18T08:33:34.300902Z"
    }
   },
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"3bedd8f0e8248cda207ca305567e02256f493396\")\n",
    "run = wandb.init(\n",
    "    project='my fine-tuning on deepseek r1 with medical data',\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kodi\\_netrc\n",
      "wandb: Currently logged in as: 819343713 (819343713-homes-com) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>E:\\AI\\LLM\\wandb\\run-20250218_163335-gi1fpe2a</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/819343713-homes-com/my%20fine-tuning%20on%20deepseek%20r1%20with%20medical%20data/runs/gi1fpe2a?apiKey=3bedd8f0e8248cda207ca305567e02256f493396' target=\"_blank\">expert-flower-7</a></strong> to <a href='https://wandb.ai/819343713-homes-com/my%20fine-tuning%20on%20deepseek%20r1%20with%20medical%20data?apiKey=3bedd8f0e8248cda207ca305567e02256f493396' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/819343713-homes-com/my%20fine-tuning%20on%20deepseek%20r1%20with%20medical%20data?apiKey=3bedd8f0e8248cda207ca305567e02256f493396' target=\"_blank\">https://wandb.ai/819343713-homes-com/my%20fine-tuning%20on%20deepseek%20r1%20with%20medical%20data?apiKey=3bedd8f0e8248cda207ca305567e02256f493396</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/819343713-homes-com/my%20fine-tuning%20on%20deepseek%20r1%20with%20medical%20data/runs/gi1fpe2a?apiKey=3bedd8f0e8248cda207ca305567e02256f493396' target=\"_blank\">https://wandb.ai/819343713-homes-com/my%20fine-tuning%20on%20deepseek%20r1%20with%20medical%20data/runs/gi1fpe2a?apiKey=3bedd8f0e8248cda207ca305567e02256f493396</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "4bea7966-4da4-463e-b4c4-cf628b74f3d2",
   "metadata": {},
   "source": "ç„¶åå¼€å§‹å¾®è°ƒï¼š"
  },
  {
   "cell_type": "code",
   "id": "cd782462-3d29-490c-9d63-4cb43a240883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T09:29:09.283124Z",
     "start_time": "2025-02-18T08:33:41.765057Z"
    }
   },
   "source": "trainer_stats = trainer.train()",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 500 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 10\n",
      " \"-____-\"     Number of trainable parameters = 40,370,176\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 50:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.379800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.826900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.881500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.621000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>9.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.108700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.249200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "a1278f09-4a99-4833-900a-659cd3e002eb",
   "metadata": {},
   "source": "æ­¤æ—¶wandbä¸­æ˜¾ç¤ºå†…å®¹å¦‚ä¸‹ï¼š"
  },
  {
   "cell_type": "markdown",
   "id": "a107a62e-eed6-49ad-8c65-7b51b12819ba",
   "metadata": {},
   "source": "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206200441907.png\" alt=\"image-20250206200441907\" style=\"zoom:50%;\" />"
  },
  {
   "cell_type": "code",
   "id": "05245a7d-6239-436d-847d-5d3188e1ec52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T10:25:59.794670Z",
     "start_time": "2025-02-18T10:25:59.790743Z"
    }
   },
   "source": "trainer_stats",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=8.202198266983032, metrics={'train_runtime': 3326.9277, 'train_samples_per_second': 0.024, 'train_steps_per_second': 0.003, 'total_flos': 2595710206580736.0, 'train_loss': 8.202198266983032, 'epoch': 0.16})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "b3fa75b2-0345-4c18-8eda-ebf1241b269b",
   "metadata": {},
   "source": "æ³¨æ„ï¼Œunslothåœ¨å¾®è°ƒç»“æŸåï¼Œä¼šè‡ªåŠ¨æ›´æ–°æ¨¡å‹æƒé‡ï¼ˆåœ¨ç¼“å­˜ä¸­ï¼‰ï¼Œå› æ­¤æ— éœ€æ‰‹åŠ¨åˆå¹¶æ¨¡å‹æƒé‡å³å¯ç›´æ¥è°ƒç”¨å¾®è°ƒåçš„æ¨¡å‹ï¼š"
  },
  {
   "cell_type": "code",
   "id": "efaf4cf4-ef13-4e3f-bc7f-d08fd36ffc4e",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-18T10:26:01.658583Z",
     "start_time": "2025-02-18T10:26:01.649071Z"
    }
   },
   "source": "FastLanguageModel.for_inference(model)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "64bca6ee-fc4c-4ca0-ab9d-d099f628f1e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:16:24.727280Z",
     "start_time": "2025-02-18T08:09:51.234307Z"
    }
   },
   "source": "inputs = tokenizer([prompt_style.format(question_1, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)",
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "5bbb2f57-8e83-484f-b8b6-e34d9d9a529f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:16:24.765272Z",
     "start_time": "2025-02-18T08:16:24.759866Z"
    }
   },
   "source": "print(response[0].split(\"### Response:\")[1])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "\n",
      "The response is based on clinical reasoning, evaluating the findings from the gynecological examination. The gynecologist's report and findings are used to assess the patient's situation. The patient's history, current condition, and results from the examination are considered. The response addresses the evaluation of the findings and their implications for the patient's overall health.\n",
      "\n",
      "The response explains the clinical reasoning, the evaluation of the findings, and their implications for the patient's condition. It also considers the patient's history and current status to provide a comprehensive assessment of their overall health.\n",
      "\n",
      "The response is structured to guide the reader through the clinical evaluation, the implications of the findings, and their relevance to the patient's condition. It emphasizes the importance of the findings in determining the patient's overall health and their potential impact on future health.\n",
      "\n",
      "The conclusion is that the findings from the gynecological examination are significant in evaluating the patient's condition and are relevant to their overall health. The patient's history and current condition are integral to this evaluation, and the response highlights the importance of these factors in determining their overall health.\n",
      "</think>\n",
      "\n",
      "The response is based on clinical reasoning and evaluates the findings from the gynecological examination. The gynecologist's report and findings are used to assess the patient's history, current condition, and results from the examination. The response addresses the evaluation of the findings and their implications for the patient's overall health. The conclusion is that the findings are significant in evaluating the patient's condition and are relevant to their overall health. The patient's history and current condition are integral to this evaluation, and the response highlights the importance of these factors in determining the patient's overall health.\n",
      "\n",
      "The response is structured to guide the reader through the clinical evaluation, the implications of the findings, and their relevance to the patient's condition. It emphasizes the importance of the findings in determining the patient's overall health and their potential impact on future health.\n",
      "\n",
      "The response highlights the importance of the findings in evaluating the patient's condition and their relevance to their overall health. The patient's history and current condition are integral to this evaluation, and the response emphasizes the importance of these factors in determining the patient's overall health.\n",
      "\n",
      "The response concludes that the findings from the gynecological examination are significant in evaluating the patient's condition and are relevant to their overall health. The patient's history and current condition are integral to this evaluation, and the response highlights the importance of these factors in determining the patient's overall health.\n",
      "\n",
      "The response is structured to guide the reader through the clinical evaluation, the implications of the findings, and their relevance to the patient's condition. It emphasizes the importance of the findings in determining the patient's overall health and their potential impact on future health.\n",
      "\n",
      "The response highlights the importance of the findings in evaluating the patient's condition and their relevance to their overall health. The patient's history and current condition are integral to this evaluation, and the response emphasizes the importance of these factors in determining the patient's overall health.\n",
      "\n",
      "The response concludes that the findings from the gynecological examination are significant in evaluating the patient's condition and are relevant to their overall health. The patient's history and current condition are integral to this evaluation, and the response highlights the importance of these factors in determining the patient's overall health.\n",
      "\n",
      "The response is structured to guide the reader through the clinical evaluation, the implications of the findings, and their relevance to the patient's condition. It emphasizes the importance of the findings in determining the patient's overall health and their potential impact on future health.\n",
      "\n",
      "The response highlights the importance of the findings in evaluating the patient's condition and their relevance to their overall health. The patient's history and current condition are integral to this evaluation, and the response emphasizes the importance of these factors in determining the patient's overall health.\n",
      "\n",
      "The response concludes that the findings from the gynecological examination are significant in evaluating the patient's condition and are relevant to their overall health. The patient's history and current condition are integral to this evaluation, and the response highlights the importance of these factors in determining the patient's overall health.\n",
      "\n",
      "The response is structured to guide the reader through the clinical evaluation, the implications of the findings, and their relevance to the patient's condition. It emphasizes the importance of the findings in determining the patient's overall health and their potential impact on future health.\n",
      "\n",
      "The response highlights the importance of the findings in evaluating the patient's condition and their relevance to their overall health. The patient's history and current condition are integral to this evaluation, and the response emphasizes the importance of these factors in determining the patient's overall health.\n",
      "\n",
      "The response concludes that the findings from the gynecological examination are significant in evaluating the patient's condition and are relevant to their overall health. The patient's history and current condition are integral to this evaluation, and the response highlights the importance of these factors in determining the patient's overall health.\n",
      "\n",
      "The response is structured to guide the reader through the clinical evaluation, the implications of the findings, and their relevance to the patient's condition. It emphasizes the importance of the findings in determining the patient's overall health and their potential impact on future health.\n",
      "\n",
      "The response highlights the importance of the findings in evaluating the patient's condition and their relevance to their overall health. The patient's history and current condition are integral to this evaluation, and the response emphasizes the importance of these factors in determining the patient's overall health.\n",
      "\n",
      "The response concludes that the findings from the gynecological examination are significant in evaluating the patient's condition and are relevant to their overall health. The patient's history and current condition are integral to this evaluation, and the response highlights the importance of these factors in determining the patient's overall health.\n",
      "\n",
      "The response is structured to guide the reader through the clinical evaluation, the implications of the findings, and their relevance to the patient's condition. It emphasizes the importance of the findings in determining the patient's overall health and their potential impact on future health.\n",
      "\n",
      "The response highlights the importance of the findings in evaluating the patient's condition and their\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "46783905-93f7-4886-9920-2ca46d836593",
   "metadata": {},
   "source": "æµ‹è¯•ç¬¬äºŒä¸ªé—®é¢˜é—®ç­”æ•ˆæœï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6b850a79-eba4-44fc-98d8-1488a85742a9",
   "metadata": {},
   "outputs": [],
   "source": "inputs = tokenizer([prompt_style.format(question_2, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c1834b90-c3e2-4a0b-b967-0293726a78a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Alright, let's think about this. We've got a patient with sudden chest pain, and it's radiating to the neck and left arm. That sounds like angina pectoris. Angina is usually due to reduced blood flow to the heart muscle. Now, I remember that there are different types of angina, and each one relates to a specific coronary artery.\n",
      "\n",
      "Let's consider what we know. The patient has hypercholesterolemia, which means there's a higher level of cholesterol in their blood. This could lead to atherosclerosis, narrowing the arteries over time. They also have coronary artery disease, which means they have some narrowing in their arteries. So, this makes it more likely that they have significant blockages.\n",
      "\n",
      "Looking at their symptoms, the pain is radiating to the neck and left arm. This sounds a lot like angina due to atherosclerosis of the left main coronary artery, which supplies the entire left ventricle. This artery is crucial because if it's blocked, it can lead to severe issues like a heart attack.\n",
      "\n",
      "Oh, and I almost forgot about the elevated troponin I levels. That's a clear sign of injury to the heart muscle. This would usually mean there's a serious blockage in the coronary artery. Given the symptoms and the medical history, it seems pretty clear that the left main coronary artery is the culprit.\n",
      "\n",
      "So, putting it all together, the most likely coronary artery involved in this situation is the left main coronary artery. It makes sense with the symptoms, the history of cholesterol issues, and the elevated troponin levels.\n",
      "</think>\n",
      "Based on the presentation of sudden chest pain radiating to the neck and left arm, along with the patient's history of hypercholesterolemia and coronary artery disease, the most likely coronary artery involved is the left main coronary artery. This artery supplies the entire left ventricle, and its blockage can lead to severe complications such as a heart attack. The elevated troponin I levels further support this conclusion, indicating cardiac injury due to a significant coronary artery blockage. Therefore, the left main coronary artery is the most likely culprit for this patient's symptoms.<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": "print(response[0].split(\"### Response:\")[1])"
  },
  {
   "cell_type": "markdown",
   "id": "aeb726b1-1e9a-45e3-a197-992e67e1e5b3",
   "metadata": {},
   "source": "> æ­¤æ—¶æ¨¡å‹è®¤ä¸ºâ€œå·¦ä¸»å† çŠ¶åŠ¨è„‰æœ€å¯èƒ½æ˜¯è¯¥æ‚£è€…ç—‡çŠ¶çš„ç½ªé­ç¥¸é¦–â€ï¼Œä½†å®é™…ä¸Šåº”è¯¥æ˜¯â€œå·¦å‰é™æ”¯ï¼ˆLADï¼‰åŠ¨è„‰å—ç´¯â€å¯¼è‡´ã€‚"
  },
  {
   "cell_type": "markdown",
   "id": "26d7c4e0-d1cd-4fc9-8dbd-c76ed581f2b3",
   "metadata": {},
   "source": "èƒ½å¤Ÿå‘ç°ï¼Œç¬¬ä¸€ä¸ªé—®é¢˜å›ç­”æ›´åŠ è§„èŒƒï¼Œå¹¶ä¸”å›ç­”æ­£ç¡®ã€‚ä½†ç¬¬äºŒä¸ªé—®é¢˜ä»ç„¶å›ç­”é”™è¯¯ã€‚ç”±æ­¤å¯ä»¥è€ƒè™‘ç»§ç»­è¿›è¡Œå¤§è§„æ¨¡å¾®è°ƒã€‚ä¸è¿‡åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç°åœ¨å°è§„æ¨¡å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæœ¬åœ°ä¿å­˜ã€‚"
  },
  {
   "cell_type": "markdown",
   "id": "83871526-b408-4090-8ea5-73ed0a7daba7",
   "metadata": {},
   "source": "#### 3.æ¨¡å‹åˆå¹¶"
  },
  {
   "cell_type": "markdown",
   "id": "2c3b846d-ebe6-4b9a-a5ff-e5971258e3fc",
   "metadata": {},
   "source": "æ­¤æ—¶æœ¬åœ°ä¿å­˜çš„æ¨¡å‹æƒé‡åœ¨`outputs`æ–‡ä»¶å¤¹ä¸­ï¼š"
  },
  {
   "cell_type": "markdown",
   "id": "f378c031-80f1-4eac-9775-7557188981ab",
   "metadata": {},
   "source": "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206195427494.png\" alt=\"image-20250206195427494\" style=\"zoom:50%;\" />"
  },
  {
   "cell_type": "markdown",
   "id": "9b5e9f53-0ee6-4830-86b9-4ba2f04c5169",
   "metadata": {},
   "source": "ç„¶åå¯ä½¿ç”¨å¦‚ä¸‹ä»£ç è¿›è¡Œæ¨¡å‹æƒé‡åˆå¹¶ï¼š"
  },
  {
   "cell_type": "code",
   "id": "15877f4c-e0c4-4ef0-ba89-70381b2d1436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T10:27:13.770080Z",
     "start_time": "2025-02-18T10:26:18.591839Z"
    }
   },
   "source": [
    "new_model_local = \"DeepSeek-R1-Medical-COT-Tiny\"\n",
    "model.save_pretrained(new_model_local) \n",
    "tokenizer.save_pretrained(new_model_local)\n",
    "\n",
    "model.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\",)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 7.75 out of 31.82 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:26<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "d3ac9133-227e-477e-9e0e-1b26c2c87480",
   "metadata": {},
   "source": "ä¿å­˜ç»“æŸåï¼Œå³å¯åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸­çœ‹åˆ°å¯¹åº”æ¨¡å‹ï¼š"
  },
  {
   "cell_type": "markdown",
   "id": "c4dbba3b-9956-42b7-9247-953f2240f640",
   "metadata": {},
   "source": "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206200056475.png\" alt=\"image-20250206200056475\" style=\"zoom:50%;\" />"
  },
  {
   "cell_type": "markdown",
   "id": "743ee184-b03c-40c5-bfc8-be63e134bfbf",
   "metadata": {},
   "source": "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206200157110.png\" alt=\"image-20250206200157110\" style=\"zoom:50%;\" />"
  },
  {
   "cell_type": "markdown",
   "id": "1f62c52e-387b-4673-b214-ab3e060a76fb",
   "metadata": {},
   "source": "ç„¶åå³å¯å°†å…¶æ¨é€åˆ°huggingfaceä¸Šå¹¶ä¿å­˜ä¸ºGGUFæ ¼å¼æ–‡ä»¶å¹¶è¿›è¡Œè°ƒç”¨ã€‚"
  },
  {
   "cell_type": "code",
   "id": "b37e85dd-7926-4716-a046-ac07ca67d038",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T10:41:49.483724Z",
     "start_time": "2025-02-18T10:40:45.135674Z"
    }
   },
   "source": [
    "# model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "# model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"q8_0\")\n",
    "model.save_pretrained_gguf(r\"E:\\AI\\LLM\\DeepSeek-R1-Medical-COT-Tiny\", tokenizer, quantization_method = \"f16\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 9.26 out of 31.82 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:39<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['f16'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: The file 'llama.cpp/llama-quantize' or 'llama.cpp/quantize' does not exist.\nBut we expect this file to exist! Maybe the llama.cpp developers changed the name?",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"q4_k_m\")\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"q8_0\")\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m model\u001B[38;5;241m.\u001B[39msave_pretrained_gguf(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mAI\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mLLM\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDeepSeek-R1-Medical-COT-Tiny\u001B[39m\u001B[38;5;124m\"\u001B[39m, tokenizer, quantization_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf16\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mE:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\unsloth\\save.py:1735\u001B[0m, in \u001B[0;36munsloth_save_pretrained_gguf\u001B[1;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001B[0m\n\u001B[0;32m   1732\u001B[0m is_sentencepiece_model \u001B[38;5;241m=\u001B[39m check_if_sentencepiece_model(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m   1734\u001B[0m \u001B[38;5;66;03m# Save to GGUF\u001B[39;00m\n\u001B[1;32m-> 1735\u001B[0m all_file_locations, want_full_precision \u001B[38;5;241m=\u001B[39m save_to_gguf(\n\u001B[0;32m   1736\u001B[0m     model_type, model_dtype, is_sentencepiece_model, \n\u001B[0;32m   1737\u001B[0m     new_save_directory, quantization_method, first_conversion, makefile,\n\u001B[0;32m   1738\u001B[0m )\n\u001B[0;32m   1740\u001B[0m \u001B[38;5;66;03m# Save Ollama modelfile\u001B[39;00m\n\u001B[0;32m   1741\u001B[0m modelfile \u001B[38;5;241m=\u001B[39m create_ollama_modelfile(tokenizer, all_file_locations[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32mE:\\miniconda3\\envs\\llama_factory\\Lib\\site-packages\\unsloth\\save.py:1070\u001B[0m, in \u001B[0;36msave_to_gguf\u001B[1;34m(model_type, model_dtype, is_sentencepiece, model_directory, quantization_method, first_conversion, _run_installer)\u001B[0m\n\u001B[0;32m   1068\u001B[0m     quantize_location \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama.cpp/llama-quantize\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1069\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1070\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1071\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnsloth: The file \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mllama.cpp/llama-quantize\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mllama.cpp/quantize\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m does not exist.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\\\n\u001B[0;32m   1072\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBut we expect this file to exist! Maybe the llama.cpp developers changed the name?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1073\u001B[0m     )\n\u001B[0;32m   1074\u001B[0m \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1076\u001B[0m \u001B[38;5;66;03m# See https://github.com/unslothai/unsloth/pull/730\u001B[39;00m\n\u001B[0;32m   1077\u001B[0m \u001B[38;5;66;03m# Filenames changed again!\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Unsloth: The file 'llama.cpp/llama-quantize' or 'llama.cpp/quantize' does not exist.\nBut we expect this file to exist! Maybe the llama.cpp developers changed the name?"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "17e7bc26-8d7a-4388-aa4b-4c48b6875f21",
   "metadata": {},
   "source": "### ä¸‰ã€å®Œæ•´é«˜æ•ˆå¾®è°ƒå®éªŒ"
  },
  {
   "cell_type": "markdown",
   "id": "7446a0c5-c8cf-45ae-a069-8a4ee40f9e60",
   "metadata": {},
   "source": "&emsp;&emsp;æ¥ä¸‹æ¥æˆ‘ä»¬å°è¯•å¸¦å…¥å…¨éƒ¨æ•°æ®è¿›è¡Œé«˜æ•ˆå¾®è°ƒï¼Œä»¥æå‡æ¨¡å‹å¾®è°ƒæ•ˆæœã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b634ea8-b9a1-4b80-adec-8ca219f862b5",
   "metadata": {},
   "outputs": [],
   "source": "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\""
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b173a851-3466-4596-903a-3e591408e412",
   "metadata": {},
   "outputs": [],
   "source": "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n\n\ndef formatting_prompts_func(examples):\n    inputs = examples[\"Question\"]\n    cots = examples[\"Complex_CoT\"]\n    outputs = examples[\"Response\"]\n    texts = []\n    for input, cot, output in zip(inputs, cots, outputs):\n        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n        texts.append(text)\n    return {\n        \"text\": texts,\n    }"
  },
  {
   "cell_type": "markdown",
   "id": "e4e5cdcf-be2a-4c69-96b7-66490e6029e1",
   "metadata": {},
   "source": "æ­¤æ—¶è¯»å–å…¨éƒ¨æ•°æ®"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd3d9b24-29e0-4868-afb4-527ae245d67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350eda58abfe4178bb86ee7308aaf3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Response:\\n<think>\\nOkay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\\n</think>\\nCystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.<ï½œendâ–ofâ–sentenceï½œ>\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train\",trust_remote_code=True)\ndataset = dataset.map(formatting_prompts_func, batched = True,)\ndataset[\"text\"][0]"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4eb8209-4f35-4551-8a8b-35aecb0eccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": "model = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  \n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    lora_alpha=16,\n    lora_dropout=0,  \n    bias=\"none\",  \n    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n    random_state=3407,\n    use_rslora=False,  \n    loftq_config=None,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "e8d7dce0-84cd-46e2-b0a7-a05e3b81b211",
   "metadata": {},
   "source": "è¿™é‡Œè®¾ç½®epochä¸º3ï¼Œéå†3æ¬¡æ•°æ®é›†ï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4739bb1a-0d51-48d2-beb3-0d6449e7ad55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3d1143312447a0ac71a3f7b0308549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/25371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        num_train_epochs = 3,\n        warmup_steps=5,\n        # max_steps=60,\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n    ),\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8576d5-8fb1-4b5d-8707-24703ff79945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 25,371 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 9,513\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='389' max='9513' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 389/9513 13:44 < 5:24:01, 0.47 it/s, Epoch 0.12/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.245400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.319800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.295300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.244500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.273400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.250100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.282500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.261500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.239800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.329700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.215600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "trainer_stats = trainer.train()"
  },
  {
   "cell_type": "markdown",
   "id": "ca87f0b5-c6cc-4885-b19d-bc3b3cd1cfae",
   "metadata": {},
   "source": "è¿™é‡Œæ€»å…±è®­ç»ƒçº¦15ä¸ªå°æ—¶ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ff611dd-e6be-4066-97d4-b288711465bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9513, training_loss=1.0824475168592858, metrics={'train_runtime': 20193.217, 'train_samples_per_second': 3.769, 'train_steps_per_second': 0.471, 'total_flos': 2.7936033274397737e+18, 'train_loss': 1.0824475168592858, 'epoch': 2.9992117294655527})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "trainer_stats"
  },
  {
   "cell_type": "markdown",
   "id": "57354364-e766-4094-b3f2-03d9f736110f",
   "metadata": {},
   "source": "å¸¦å…¥ä¸¤ä¸ªé—®é¢˜è¿›è¡Œæµ‹è¯•ï¼Œå‡æœ‰è¾ƒå¥½çš„å›ç­”æ•ˆæœï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7893ec0-c557-4347-91f0-af1a28163451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Alright, let's think this through step by step. We've got a 61-year-old woman who's been dealing with involuntary urine loss whenever she does something like coughing or sneezing. That sounds like stress urinary incontinence, which usually means there's some kind of leakage when there's increased pressure in the abdomen. Now, the fact that she doesn't have any leakage at night is interesting. That's a big hint because it suggests that the problem isn't about bladder control or retention, since there's no issue when lying down.\n",
      "\n",
      "Now, let's consider the Q-tip test. This test is a clever way to check for urethral mobility. If the Q-tip moves a lot, it means the urethra is mobile, which is a typical sign of stress incontinence. So, we're probably looking at a condition where the urethra isn't staying closed under pressure.\n",
      "\n",
      "Now, if we were to do a cystometry on her, we'd be checking a few things. First, there's the residual volume. Since she's not leaking urine at night, her bladder should be empty, so we'd expect the residual volume to be normal. Then, there's the detrusor contractions. Stress incontinence doesn't usually involve the detrusor muscle contracting abnormally. So, we'd expect normal or maybe slightly increased contractions, but nothing out of the ordinary.\n",
      "\n",
      "So, putting it all together, if we do the cystometry, we'd likely see a normal residual volume and normal detrusor contractions. That fits with everything we know about her symptoms and the Q-tip test results. Everything lines up with stress urinary incontinence, confirming that our conclusion makes sense.\n",
      "</think>\n",
      "Based on the information provided, the 61-year-old woman is likely experiencing stress urinary incontinence, as suggested by the involuntary urine loss during activities like coughing or sneezing, coupled with the fact that she doesn't experience leakage at night. During a cystometry exam, we would expect the following findings:\n",
      "\n",
      "1. **Residual Volume:** This would be normal. Since she does not experience leakage at night, her bladder is likely empty at bedtime, indicating a normal post-void residual volume.\n",
      "\n",
      "2. **Detrusor Contractions:** These would also be normal or possibly slightly increased. Stress urinary incontinence typically does not involve abnormal detrusor contractions, as the issue is more related to the closure mechanism of the urethra under increased abdominal pressure rather than bladder control or contractions.\n",
      "\n",
      "Thus, cystometry would most likely reveal a normal residual volume and normal detrusor contractions, consistent with stress urinary incontinence.<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": "question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\n\n\nFastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nprint(response[0].split(\"### Response:\")[1])"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8bba1c2-a7c7-4e82-9795-48a117c70c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, let's think about this. The patient is having sudden chest pain that's shooting to their neck and left arm. That's pretty classic for something like a myocardial infarction, right? I mean, the pain spreading like that usually points to a big problem in the heart's supply.\n",
      "\n",
      "Now, considering the heart's coronary arteries, I know the left anterior descending artery, or LAD, is a big player. It's like the main highway for blood to reach the front wall of the heart. If there's a blockage there, it can definitely cause pain that radiates to the neck and arm.\n",
      "\n",
      "Then, there's the right coronary artery, or RCA, which supplies the right side of the heart and can affect the inferior wall of the heart. But wait, the pain pattern here seems to be more on the left side, so maybe the RCA is less likely.\n",
      "\n",
      "The patient has hypercholesterolemia and coronary artery disease. These conditions put them at risk for atherosclerosis, which can lead to blockages in the coronary arteries. The LAD is commonly involved in such scenarios, especially when the pain spreads to the neck and arm.\n",
      "\n",
      "Also, the elevated troponin I levels and tachycardia are strong signals that something serious is happening in the heart. These are usually seen in myocardial infarctions. Given the pain pattern and the patient's risk factors, the LAD seems like the most likely culprit here.\n",
      "\n",
      "So, when I put all this together, it really seems like the left anterior descending artery is the most likely artery involved in this situation. It just fits with the classic presentation of anterior myocardial infarction. Yeah, I'm pretty confident about that.\n",
      "</think>\n",
      "Based on the presentation of sudden-onset chest pain radiating to the neck and left arm, along with the patient's history of hypercholesterolemia and coronary artery disease, the most likely coronary artery involved is the left anterior descending (LAD) artery. This artery supplies the front wall of the heart, and a blockage here can cause the classic symptoms described. The elevated troponin I levels and tachycardia further support the likelihood of a myocardial infarction, with the LAD being a common site for such events.<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": "question = \"Given a patient who experiences sudden-onset chest pain radiating to the neck and left arm, with a past medical history of hypercholesterolemia and coronary artery disease, elevated troponin I levels, and tachycardia, what is the most likely coronary artery involved based on this presentation?\"\n\n\nFastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nprint(response[0].split(\"### Response:\")[1])"
  },
  {
   "cell_type": "markdown",
   "id": "e19b0182-51de-4e71-a15a-003d56c1da7f",
   "metadata": {},
   "source": "æœ€åè¿›è¡Œæ¨¡å‹æƒé‡ä¿å­˜ï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "099775b8-7ff0-4dee-b422-a72bc1d5a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 712.42 out of 1007.51 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 57.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": "new_model_local = \"DeepSeek-R1-Medical-COT\"\nmodel.save_pretrained(new_model_local) \ntokenizer.save_pretrained(new_model_local)\n\nmodel.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\",)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
